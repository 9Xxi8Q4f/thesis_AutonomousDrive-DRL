{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 16:18:27.014995: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-28 16:18:27.296339: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-28 16:18:27.296405: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-28 16:18:27.297671: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-28 16:18:27.411638: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# fundemental modules\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import pprint\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from ddqn import DDQNAgent # type: ignore\n",
    "from utils import record_videos, load_config, show_videos\n",
    "from reward import _reward\n",
    "from obs import observation_shape\n",
    "\n",
    "aggregate_stats_every=100\n",
    "best_score = -1000.0\n",
    "score_history = []\n",
    "episode_lens = []\n",
    "avg_history = []\n",
    "std_history = []\n",
    "avg_history_100 = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/o/Documents/thesis/.venv/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.configure to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.configure` for environment variables or `env.get_wrapper_attr('configure')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "data = load_config()\n",
    "env = gym.make('racetrack-v0', render_mode = 'rgb_array')\n",
    "env.configure(data) # type: ignore\n",
    "# pprint.pprint(env.config) # type: ignore\n",
    "(obs, info), done = env.reset(), False\n",
    "\n",
    "#observation config\n",
    "proc = observation_shape(obs,info,2)\n",
    "proc.reset()\n",
    "input = proc.get_input()\n",
    "print(input.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent is initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 16:18:33.592216: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-28 16:18:33.713679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-28 16:18:33.713896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-28 16:18:33.715177: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-28 16:18:33.715347: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-28 16:18:33.715477: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-28 16:18:34.313311: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-28 16:18:34.313407: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-28 16:18:34.313462: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-28 16:18:34.313506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9738 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "agent = DDQNAgent(alpha=0.001, gamma=0.99, epsilon=1.0, obs_shape=input.shape,\n",
    "                  batch_size=128, epsilon_dec=0.9993, epsilon_end=0.05, mem_size=100000,\n",
    "                  min_mem_size=150, replace_target=1000, learning_rate=0.0007)\n",
    "print(\"Agent is initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31c7db6d6ce4d9f96e826b11098a5b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test episodes:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score 17.9 ep len 33 avg score 30.3 avg_score_100 64.3 std score 30.6\n",
      "-----saving models------\n",
      "episode  1 score 108.4 ep len 151 avg score 30.6 avg_score_100 65.0 std score 30.9\n",
      "episode  2 score 12.7 ep len 18 avg score 30.5 avg_score_100 64.8 std score 30.9\n",
      "-----saving models------\n",
      "episode  3 score 105.9 ep len 145 avg score 30.8 avg_score_100 65.3 std score 31.1\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  4 score 122.0 ep len 160 avg score 31.1 avg_score_100 66.0 std score 31.5\n",
      "-----saving models------\n",
      "episode  5 score 139.7 ep len 190 avg score 31.4 avg_score_100 66.8 std score 32.1\n",
      "-----saving models------\n",
      "episode  6 score 102.9 ep len 132 avg score 31.6 avg_score_100 67.3 std score 32.3\n",
      "episode  7 score 15.5 ep len 23 avg score 31.6 avg_score_100 67.0 std score 32.2\n",
      "episode  8 score 8.4 ep len 15 avg score 31.5 avg_score_100 66.6 std score 32.2\n",
      "episode  9 score 10.3 ep len 16 avg score 31.4 avg_score_100 66.2 std score 32.2\n",
      "episode  10 score 20.4 ep len 28 avg score 31.4 avg_score_100 65.8 std score 32.1\n",
      "episode  11 score 16.8 ep len 26 avg score 31.4 avg_score_100 65.4 std score 32.1\n",
      "-----saving models------\n",
      "episode  12 score 118.3 ep len 159 avg score 31.6 avg_score_100 66.3 std score 32.4\n",
      "-----saving models------\n",
      "episode  13 score 61.6 ep len 85 avg score 31.7 avg_score_100 66.2 std score 32.4\n",
      "episode  14 score 15.0 ep len 20 avg score 31.7 avg_score_100 65.8 std score 32.4\n",
      "episode  15 score 36.0 ep len 50 avg score 31.7 avg_score_100 65.6 std score 32.3\n",
      "episode  16 score 26.0 ep len 40 avg score 31.7 avg_score_100 65.4 std score 32.3\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  17 score 164.5 ep len 225 avg score 32.1 avg_score_100 66.5 std score 33.1\n",
      "-----saving models------\n",
      "episode  18 score 131.7 ep len 181 avg score 32.4 avg_score_100 67.3 std score 33.5\n",
      "episode  19 score 21.2 ep len 31 avg score 32.4 avg_score_100 66.9 std score 33.4\n",
      "-----saving models------\n",
      "episode  20 score 139.6 ep len 200 avg score 32.7 avg_score_100 67.8 std score 33.9\n",
      "-----saving models------\n",
      "episode  21 score 61.0 ep len 76 avg score 32.8 avg_score_100 67.8 std score 33.9\n",
      "-----saving models------\n",
      "episode  22 score 139.6 ep len 194 avg score 33.1 avg_score_100 68.6 std score 34.4\n",
      "episode  23 score 14.0 ep len 24 avg score 33.1 avg_score_100 68.1 std score 34.3\n",
      "-----saving models------\n",
      "episode  24 score 106.4 ep len 138 avg score 33.3 avg_score_100 68.5 std score 34.5\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  25 score 179.9 ep len 234 avg score 33.7 avg_score_100 69.8 std score 35.4\n",
      "-----saving models------\n",
      "episode  26 score 125.9 ep len 151 avg score 34.0 avg_score_100 70.4 std score 35.7\n",
      "-----saving models------\n",
      "episode  27 score 131.9 ep len 169 avg score 34.3 avg_score_100 71.3 std score 36.1\n",
      "-----saving models------\n",
      "episode  28 score 134.0 ep len 175 avg score 34.6 avg_score_100 72.1 std score 36.4\n",
      "-----saving models------\n",
      "episode  29 score 75.1 ep len 94 avg score 34.8 avg_score_100 72.2 std score 36.4\n",
      "-----saving models------\n",
      "episode  30 score 41.6 ep len 57 avg score 34.8 avg_score_100 72.1 std score 36.4\n",
      "-----saving models------\n",
      "episode  31 score 180.4 ep len 232 avg score 35.2 avg_score_100 73.4 std score 37.2\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  32 score 57.8 ep len 71 avg score 35.3 avg_score_100 73.3 std score 37.2\n",
      "-----saving models------\n",
      "episode  33 score 145.8 ep len 185 avg score 35.6 avg_score_100 74.1 std score 37.6\n",
      "-----saving models------\n",
      "episode  34 score 58.0 ep len 74 avg score 35.7 avg_score_100 74.1 std score 37.6\n",
      "-----saving models------\n",
      "episode  35 score 114.8 ep len 138 avg score 35.9 avg_score_100 74.7 std score 37.7\n",
      "-----saving models------\n",
      "episode  36 score 124.1 ep len 146 avg score 36.2 avg_score_100 75.3 std score 38.0\n",
      "-----saving models------\n",
      "episode  37 score 168.8 ep len 235 avg score 36.6 avg_score_100 76.3 std score 38.6\n",
      "-----saving models------\n",
      "episode  38 score 125.2 ep len 156 avg score 36.8 avg_score_100 76.9 std score 38.9\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  39 score 62.6 ep len 75 avg score 36.9 avg_score_100 77.0 std score 38.8\n",
      "-----saving models------\n",
      "episode  40 score 128.2 ep len 174 avg score 37.2 avg_score_100 77.5 std score 39.1\n",
      "-----saving models------\n",
      "episode  41 score 130.6 ep len 162 avg score 37.4 avg_score_100 78.1 std score 39.3\n",
      "-----saving models------\n",
      "episode  42 score 151.2 ep len 211 avg score 37.8 avg_score_100 79.0 std score 39.8\n",
      "-----saving models------\n",
      "episode  43 score 120.8 ep len 153 avg score 38.0 avg_score_100 79.5 std score 40.0\n",
      "-----saving models------\n",
      "episode  44 score 62.7 ep len 78 avg score 38.1 avg_score_100 79.6 std score 39.9\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  45 score 160.0 ep len 211 avg score 38.4 avg_score_100 80.9 std score 40.4\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  46 score 58.2 ep len 69 avg score 38.5 avg_score_100 80.8 std score 40.4\n",
      "-----saving models------\n",
      "episode  47 score 124.3 ep len 152 avg score 38.7 avg_score_100 81.5 std score 40.6\n",
      "episode  48 score 16.9 ep len 25 avg score 38.7 avg_score_100 81.1 std score 40.5\n",
      "-----saving models------\n",
      "episode  49 score 143.9 ep len 201 avg score 39.0 avg_score_100 82.0 std score 40.8\n",
      "-----saving models------\n",
      "episode  50 score 187.3 ep len 243 avg score 39.4 avg_score_100 83.3 std score 41.5\n",
      "-----saving models------\n",
      "episode  51 score 158.6 ep len 207 avg score 39.7 avg_score_100 84.4 std score 42.0\n",
      "episode  52 score 8.3 ep len 12 avg score 39.7 avg_score_100 83.9 std score 41.9\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  53 score 176.8 ep len 227 avg score 40.0 avg_score_100 85.2 std score 42.5\n",
      "-----saving models------\n",
      "episode  54 score 64.6 ep len 85 avg score 40.1 avg_score_100 85.2 std score 42.5\n",
      "-----saving models------\n",
      "episode  55 score 127.7 ep len 166 avg score 40.4 avg_score_100 85.9 std score 42.7\n",
      "-----saving models------\n",
      "episode  56 score 58.3 ep len 84 avg score 40.4 avg_score_100 85.3 std score 42.6\n",
      "-----saving models------\n",
      "episode  57 score 171.9 ep len 244 avg score 40.8 avg_score_100 85.9 std score 43.1\n",
      "-----saving models------\n",
      "episode  58 score 114.0 ep len 151 avg score 41.0 avg_score_100 86.5 std score 43.2\n",
      "episode  59 score 8.0 ep len 14 avg score 40.9 avg_score_100 86.1 std score 43.2\n",
      "-----saving models------\n",
      "episode  60 score 107.5 ep len 133 avg score 41.1 avg_score_100 86.5 std score 43.3\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "episode  61 score 24.1 ep len 37 avg score 41.0 avg_score_100 85.7 std score 43.2\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "episode  62 score 32.6 ep len 39 avg score 41.0 avg_score_100 85.8 std score 43.2\n",
      "-----saving models------\n",
      "episode  63 score 142.7 ep len 178 avg score 41.3 avg_score_100 86.2 std score 43.4\n",
      "-----saving models------\n",
      "episode  64 score 196.0 ep len 253 avg score 41.7 avg_score_100 88.1 std score 44.1\n",
      "-----saving models------\n",
      "episode  65 score 136.8 ep len 168 avg score 42.0 avg_score_100 88.8 std score 44.4\n",
      "episode  66 score 40.1 ep len 50 avg score 42.0 avg_score_100 88.2 std score 44.3\n",
      "episode  67 score 6.2 ep len 11 avg score 41.9 avg_score_100 87.7 std score 44.3\n",
      "-----saving models------\n",
      "episode  68 score 163.5 ep len 220 avg score 42.2 avg_score_100 88.3 std score 44.7\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  69 score 190.0 ep len 249 avg score 42.6 avg_score_100 89.6 std score 45.3\n",
      "-----saving models------\n",
      "episode  70 score 178.4 ep len 235 avg score 43.0 avg_score_100 90.3 std score 45.7\n",
      "-----saving models------\n",
      "episode  71 score 183.8 ep len 235 avg score 43.3 avg_score_100 91.6 std score 46.3\n",
      "-----saving models------\n",
      "episode  72 score 42.3 ep len 51 avg score 43.3 avg_score_100 90.9 std score 46.2\n",
      "-----saving models------\n",
      "episode  73 score 138.5 ep len 178 avg score 43.6 avg_score_100 91.6 std score 46.4\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  74 score 181.2 ep len 238 avg score 44.0 avg_score_100 92.2 std score 46.9\n",
      "-----saving models------\n",
      "episode  75 score 184.3 ep len 231 avg score 44.3 avg_score_100 93.5 std score 47.4\n",
      "episode  76 score 21.9 ep len 28 avg score 44.3 avg_score_100 92.6 std score 47.3\n",
      "-----saving models------\n",
      "episode  77 score 173.8 ep len 224 avg score 44.6 avg_score_100 94.2 std score 47.7\n",
      "-----saving models------\n",
      "episode  78 score 172.0 ep len 219 avg score 44.9 avg_score_100 95.6 std score 48.1\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  79 score 210.2 ep len 261 avg score 45.4 avg_score_100 96.6 std score 48.8\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  80 score 204.2 ep len 261 avg score 45.8 avg_score_100 98.0 std score 49.4\n",
      "-----saving models------\n",
      "episode  81 score 125.1 ep len 147 avg score 46.0 avg_score_100 98.2 std score 49.5\n",
      "-----saving models------\n",
      "episode  82 score 129.2 ep len 161 avg score 46.2 avg_score_100 98.3 std score 49.6\n",
      "-----saving models------\n",
      "episode  83 score 169.3 ep len 227 avg score 46.5 avg_score_100 98.8 std score 49.9\n",
      "-----saving models------\n",
      "episode  84 score 59.9 ep len 72 avg score 46.6 avg_score_100 98.5 std score 49.9\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  85 score 189.0 ep len 220 avg score 46.9 avg_score_100 100.3 std score 50.3\n",
      "-----saving models------\n",
      "episode  86 score 131.1 ep len 148 avg score 47.2 avg_score_100 101.5 std score 50.5\n",
      "episode  87 score 34.9 ep len 44 avg score 47.1 avg_score_100 101.0 std score 50.4\n",
      "-----saving models------\n",
      "episode  88 score 209.0 ep len 258 avg score 47.5 avg_score_100 102.5 std score 51.0\n",
      "-----saving models------\n",
      "episode  89 score 67.7 ep len 86 avg score 47.6 avg_score_100 102.0 std score 50.9\n",
      "-----saving models------\n",
      "episode  90 score 110.1 ep len 141 avg score 47.8 avg_score_100 103.0 std score 51.0\n",
      "-----saving models------\n",
      "episode  91 score 167.3 ep len 213 avg score 48.1 avg_score_100 103.3 std score 51.3\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  92 score 126.1 ep len 156 avg score 48.3 avg_score_100 104.4 std score 51.3\n",
      "-----saving models------\n",
      "episode  93 score 182.7 ep len 245 avg score 48.6 avg_score_100 106.2 std score 51.7\n",
      "-----saving models------\n",
      "episode  94 score 45.9 ep len 56 avg score 48.6 avg_score_100 106.6 std score 51.7\n",
      "-----saving models------\n",
      "episode  95 score 223.9 ep len 278 avg score 49.0 avg_score_100 107.7 std score 52.3\n",
      "-----saving models------\n",
      "episode  96 score 186.7 ep len 244 avg score 49.4 avg_score_100 108.5 std score 52.7\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  97 score 194.5 ep len 238 avg score 49.8 avg_score_100 109.2 std score 53.2\n",
      "episode  98 score 23.9 ep len 32 avg score 49.7 avg_score_100 108.4 std score 53.1\n",
      "-----saving models------\n",
      "episode  99 score 194.6 ep len 238 avg score 50.1 avg_score_100 109.2 std score 53.5\n"
     ]
    }
   ],
   "source": [
    "# main loop\n",
    "for episode in trange(100, desc='Test episodes'):\n",
    "\n",
    "        # proc.reset()\n",
    "        (observation, info), done = env.reset(), False\n",
    "        proc.update_input(observation, info)\n",
    "        observation = proc.get_input()\n",
    "        # observation = observation.flatten()\n",
    "\n",
    "        episode_reward = 0\n",
    "        episode_len = 0\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            action, action_index = agent.get_action(observation, deterministic=False)\n",
    "            new_observation, reward, done, truncated, new_info = env.step(action=[action])\n",
    "            # reward = _reward(new_info,new_observation)\n",
    "            # new_observation = new_observation.flatten()\n",
    "            \n",
    "            proc.update_input(new_observation, info)\n",
    "            new_observation = proc.get_input()\n",
    "\n",
    "            episode_reward += reward # type: ignore\n",
    "            episode_len +=1\n",
    "            \n",
    "            agent.remember(state=observation, action=action_index, done=done,\n",
    "                            reward=reward, new_state=new_observation)\n",
    "            agent.train()\n",
    "\n",
    "            observation = new_observation\n",
    "\n",
    "        episode_lens.append(episode_len)\n",
    "\n",
    "        score_history.append(episode_reward)\n",
    "        avg_score = np.mean(score_history)\n",
    "        avg_history.append(avg_score)\n",
    "        std_score = np.std(score_history)\n",
    "        std_history.append(std_score)\n",
    "\n",
    "        avg_score_100 = np.mean(score_history[-100:])\n",
    "        avg_history_100.append(avg_score_100)\n",
    "\n",
    "        if avg_score_100 > best_score:\n",
    "            best_score = avg_score\n",
    "            agent.save_model(episode)\n",
    "\n",
    "        agent.tensorboard.update_stats(episode_rew = episode_reward,\n",
    "                                       average_rew =avg_score,\n",
    "                                       average_100_reward = avg_score_100,\n",
    "                                       std_rew=std_score,\n",
    "                                       epsilon=agent.epsilon,\n",
    "                                       episode_len = episode_len)\n",
    "        \n",
    "        #* save things in checkpoints\n",
    "        if not episode % aggregate_stats_every or episode == 1:\n",
    "            average_reward = sum(score_history[-aggregate_stats_every:])/len(score_history[-aggregate_stats_every:])\n",
    "            min_reward = min(score_history[-aggregate_stats_every:])\n",
    "            max_reward = max(score_history[-aggregate_stats_every:])\n",
    "\n",
    "            agent.tensorboard_steps.update_stats(reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward, epsilon=agent.epsilon)\n",
    "\n",
    "        print('episode ', episode, 'score %.1f' % episode_reward, 'ep len', episode_len,\n",
    "              'avg score %.1f' % avg_score, 'avg_score_100 %.1f' %avg_score_100,'std score %.1f' % std_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-----saving models------\n"
     ]
    }
   ],
   "source": [
    "n_steps = sum(episode_lens)\n",
    "print(n_steps)\n",
    "agent.save_model(100000000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----loading models------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/o/Documents/thesis/.venv/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.configure to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.configure` for environment variables or `env.get_wrapper_attr('configure')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/o/Documents/thesis/.venv/lib/python3.10/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/o/Documents/thesis/ddqn/racetrack_ppo/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79dadff19ae145cd9b261a93ab84532f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/o/Documents/thesis/.venv/lib/python3.10/site-packages/gymnasium/wrappers/monitoring/video_recorder.py:178: UserWarning: \u001b[33mWARN: Unable to save last video! Did you call close()?\u001b[0m\n",
      "  logger.warn(\"Unable to save last video! Did you call close()?\")\n"
     ]
    }
   ],
   "source": [
    "# env = record_videos(env)\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "data = load_config()\n",
    "env = gym.make('racetrack-v0', render_mode = 'rgb_array')\n",
    "env.configure(data) # type: ignore\n",
    "\n",
    "env = RecordVideo(env, video_folder=\"videos\", episode_trigger=lambda e: True)\n",
    "env.unwrapped.set_record_video_wrapper(env)\n",
    "\n",
    "agent.load_model()\n",
    "# main loop\n",
    "for episode in trange(3, desc='Test episodes'):\n",
    "\n",
    "        (observation, info), done = env.reset(), False\n",
    "        proc.update_input(observation, info)\n",
    "        observation = proc.get_input()\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            action, action_index = agent.get_action(observation, deterministic=True)\n",
    "            new_observation, reward, done, truncated, new_info = env.step(action=[action])\n",
    "            # new_observation = new_observation.flatten()\n",
    "            \n",
    "            proc.update_input(new_observation, info)\n",
    "            new_observation = proc.get_input()\n",
    "            \n",
    "            if new_info[\"rewards\"][\"on_road_reward\"] == False:\n",
    "                 done = True\n",
    "            observation = new_observation\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/o/Documents/thesis/ddqn/racetrack_ppo/videos/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/o/Documents/thesis/ddqn/racetrack_ppo/videos/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/o/Documents/thesis/ddqn/racetrack_ppo/videos/rl-video-episode-0.mp4\n"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
