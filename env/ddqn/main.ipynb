{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fundemental modules\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import pprint\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from ddqn import DDQNAgent # type: ignore\n",
    "from utils import record_videos, load_config, show_videos\n",
    "from reward import _reward\n",
    "from obs import observation_shape\n",
    "\n",
    "best_score = -1000.0\n",
    "score_history = []\n",
    "episode_lens = []\n",
    "avg_history = []\n",
    "std_history = []\n",
    "avg_history_100 = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_config()\n",
    "env = gym.make('racetrack-v0', render_mode = 'rgb_array')\n",
    "env.configure(data) # type: ignore\n",
    "# pprint.pprint(env.config) # type: ignore\n",
    "(obs, info), done = env.reset(), False\n",
    "# input = obs.flatten()\n",
    "#observation config\n",
    "proc = observation_shape(obs,info,2)\n",
    "proc.reset()\n",
    "input = proc.get_input()\n",
    "print(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDQNAgent(alpha=0.001, gamma=0.9, epsilon=1.0, obs_shape=input.shape,\n",
    "                  batch_size=512, epsilon_dec=0.9995, epsilon_end=0.1, mem_size=100000,\n",
    "                  min_mem_size=600, replace_target=1000, learning_rate=0.0003)\n",
    "print(\"Agent is initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main loop\n",
    "for episode in trange(1000, desc='Test episodes'):\n",
    "\n",
    "        # proc.reset()\n",
    "        (observation, info), done = env.reset(), False\n",
    "        proc.update_input(observation, info)\n",
    "        observation = proc.get_input()\n",
    "        # observation = observation.flatten()\n",
    "\n",
    "        episode_reward = 0\n",
    "        episode_len = 0\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            if episode%100 == 0:\n",
    "                 env.render()\n",
    "            \n",
    "            action, action_index = agent.get_action(observation, deterministic=False)\n",
    "            new_observation, reward, done, truncated, new_info = env.step(action=[action])\n",
    "            reward = _reward(new_info,new_observation)\n",
    "            # new_observation = new_observation.flatten()\n",
    "            \n",
    "            proc.update_input(new_observation, info)\n",
    "            new_observation = proc.get_input()\n",
    "\n",
    "            episode_reward += reward # type: ignore\n",
    "            episode_len +=1\n",
    "            \n",
    "            if new_info[\"rewards\"][\"on_road_reward\"] == False:\n",
    "                 done = True\n",
    "\n",
    "            agent.remember(state=observation, action=action_index, done=done,\n",
    "                            reward=reward, new_state=new_observation)\n",
    "            agent.train()\n",
    "\n",
    "            observation = new_observation\n",
    "\n",
    "        episode_lens.append(episode_len)\n",
    "\n",
    "        score_history.append(episode_reward)\n",
    "        avg_score = np.mean(score_history)\n",
    "        avg_history.append(avg_score)\n",
    "        std_score = np.std(score_history)\n",
    "        std_history.append(std_score)\n",
    "\n",
    "        avg_score_100 = np.mean(score_history[-100:])\n",
    "        avg_history_100.append(avg_score_100)\n",
    "\n",
    "        if avg_score_100 > best_score:\n",
    "            best_score = avg_score\n",
    "            agent.save_model(episode)\n",
    "\n",
    "        agent.tensorboard.update_stats(episode_rew = episode_reward,\n",
    "                                       average_rew =avg_score,\n",
    "                                       average_100_reward = avg_score_100,\n",
    "                                       std_rew=std_score,\n",
    "                                       epsilon=agent.epsilon,\n",
    "                                       episode_len = episode_len)\n",
    "\n",
    "        print(\"Last Info: \", new_info)\n",
    "        print(\"Last Reward: \", reward)\n",
    "\n",
    "        print('episode ', episode, 'score %.1f' % episode_reward, 'ep len', episode_len,\n",
    "              'avg score %.1f' % avg_score, 'avg_score_100 %.1f' %avg_score_100,'std score %.1f' % std_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = record_videos(env)\n",
    "\n",
    "agent.load_model()\n",
    "# main loop\n",
    "for episode in trange(1, desc='Test episodes'):\n",
    "        print(\"RESETTED\")\n",
    "        # proc.reset()\n",
    "        (observation, info), done = env.reset(), False\n",
    "        # proc.update_input(observation, info)\n",
    "        # observation = proc.get_input()\n",
    "        observation = observation.flatten()\n",
    "\n",
    "        episode_reward = 0\n",
    "        episode_len = 0\n",
    "\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            action, action_index = agent.get_action(observation, deterministic=True)\n",
    "            new_observation, reward, done, truncated, new_info = env.step(action=[action])\n",
    "            reward = _reward(new_info,new_observation)\n",
    "\n",
    "            new_observation = new_observation.flatten()\n",
    "            \n",
    "            # proc.update_input(new_observation, info)\n",
    "            # new_observation = proc.get_input()\n",
    "\n",
    "            episode_reward += reward # type: ignore\n",
    "            episode_len +=1\n",
    "            \n",
    "            if new_info[\"rewards\"][\"on_road_reward\"] == False:\n",
    "                 done = True\n",
    "\n",
    "            observation = new_observation\n",
    "\n",
    "        print(\"Last Observation: \", observation.reshape((2,(len(observation)+1)//2)))\n",
    "        print(\"Last Info: \", new_info)\n",
    "        print(\"Last Reward: \", reward)\n",
    "\n",
    "env.close()\n",
    "show_videos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
