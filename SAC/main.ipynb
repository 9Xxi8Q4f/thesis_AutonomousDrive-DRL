{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 22:31:10.874785: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-10 22:31:10.899395: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-10 22:31:10.899416: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-10 22:31:10.899433: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-10 22:31:10.904036: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# fundemental modules\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import pprint\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from sac import Agent\n",
    "from obs import observation_shape\n",
    "from utils import record_videos, load_config\n",
    "\n",
    "filename = 'inverted_pendulum.png'\n",
    "\n",
    "best_score = -10000.0\n",
    "best_score = -1000.0\n",
    "score_history = []\n",
    "episode_lens = []\n",
    "avg_history = []\n",
    "std_history = []\n",
    "avg_history_100 = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/o/Documents/thesis/.venv/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.configure to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.configure` for environment variables or `env.get_wrapper_attr('configure')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "data = load_config()\n",
    "env = gym.make('racetrack-v0', render_mode = 'rgb_array')\n",
    "env.configure(data) # type: ignore\n",
    "# pprint.pprint(env.config) # type: ignore\n",
    "(obs, info), done = env.reset(), False\n",
    "\n",
    "#observation config\n",
    "proc = observation_shape(obs,info,2)\n",
    "proc.reset()\n",
    "input = proc.get_input()\n",
    "print(input.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 22:31:17.584148: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 22:31:17.607584: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 22:31:17.607686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 22:31:17.608474: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 22:31:17.608603: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 22:31:17.608658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 22:31:17.958131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 22:31:17.958224: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 22:31:17.958284: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 22:31:17.958329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9793 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(input_dims=input.shape, env=env,\n",
    "            n_actions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8efd3e8637bc45159543f132f50cb969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test episodes:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 22:31:51.703190: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score -0.3 ep len 5 avg score -0.3 avg_score_100 -0.3 std score 0.0\n",
      "episode  1 score -0.3 ep len 5 avg score -0.3 avg_score_100 -0.3 std score 0.0\n",
      "episode  2 score -0.3 ep len 5 avg score -0.3 avg_score_100 -0.3 std score 0.0\n",
      "episode  3 score -0.3 ep len 5 avg score -0.3 avg_score_100 -0.3 std score 0.0\n",
      "episode  4 score -0.3 ep len 5 avg score -0.3 avg_score_100 -0.3 std score 0.0\n",
      "episode  5 score -0.3 ep len 5 avg score -0.3 avg_score_100 -0.3 std score 0.0\n",
      "episode  6 score -0.3 ep len 5 avg score -0.3 avg_score_100 -0.3 std score 0.0\n",
      "episode  7 score -0.3 ep len 5 avg score -0.3 avg_score_100 -0.3 std score 0.0\n",
      "episode  8 score -0.3 ep len 5 avg score -0.3 avg_score_100 -0.3 std score 0.0\n",
      "episode  9 score -0.3 ep len 5 avg score -0.3 avg_score_100 -0.3 std score 0.0\n",
      "episode  10 score -0.3 ep len 5 avg score -0.3 avg_score_100 -0.3 std score 0.0\n",
      "episode  11 score -0.3 ep len 5 avg score -0.3 avg_score_100 -0.3 std score 0.0\n",
      "episode  12 score -0.3 ep len 5 avg score -0.3 avg_score_100 -0.3 std score 0.0\n",
      "episode  13 score -0.3 ep len 5 avg score -0.3 avg_score_100 -0.3 std score 0.0\n",
      "episode  14 score -0.3 ep len 5 avg score -0.3 avg_score_100 -0.3 std score 0.0\n",
      "episode  15 score -0.3 ep len 5 avg score -0.3 avg_score_100 -0.3 std score 0.0\n",
      "episode  16 score -0.3 ep len 5 avg score -0.3 avg_score_100 -0.3 std score 0.0\n",
      "episode  17 score -0.3 ep len 5 avg score -0.3 avg_score_100 -0.3 std score 0.0\n",
      "episode  18 score -0.3 ep len 5 avg score -0.3 avg_score_100 -0.3 std score 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 22:31:54.535414: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e4a3b0b880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-10 22:31:54.535432: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2023-12-10 22:31:54.538125: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-12-10 22:31:55.435523: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8902\n",
      "2023-12-10 22:31:55.490312: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f1daa185240> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f1daa185240> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "episode  19 score -0.3 ep len 5 avg score -0.3 avg_score_100 -0.3 std score 0.0\n",
      "episode  20 score -0.3 ep len 5 avg score -0.3 avg_score_100 -0.3 std score 0.0\n",
      "episode  21 score -0.1 ep len 2 avg score -0.3 avg_score_100 -0.3 std score 0.0\n",
      "episode  22 score 3.7 ep len 24 avg score -0.1 avg_score_100 -0.1 std score 0.8\n",
      "episode  23 score 0.4 ep len 6 avg score -0.1 avg_score_100 -0.1 std score 0.8\n",
      "episode  24 score 0.3 ep len 3 avg score -0.0 avg_score_100 -0.0 std score 0.8\n",
      "episode  25 score 3.0 ep len 19 avg score 0.1 avg_score_100 0.1 std score 1.0\n",
      "episode  26 score 3.5 ep len 8 avg score 0.2 avg_score_100 0.2 std score 1.2\n",
      "episode  27 score 0.9 ep len 8 avg score 0.2 avg_score_100 0.2 std score 1.1\n",
      "episode  28 score 0.9 ep len 9 avg score 0.2 avg_score_100 0.2 std score 1.1\n",
      "episode  29 score 0.5 ep len 7 avg score 0.3 avg_score_100 0.3 std score 1.1\n",
      "episode  30 score -0.3 ep len 3 avg score 0.2 avg_score_100 0.2 std score 1.1\n",
      "episode  31 score 0.4 ep len 15 avg score 0.2 avg_score_100 0.2 std score 1.1\n",
      "episode  32 score 3.7 ep len 20 avg score 0.4 avg_score_100 0.4 std score 1.2\n",
      "episode  33 score 0.3 ep len 8 avg score 0.3 avg_score_100 0.3 std score 1.2\n",
      "episode  34 score 1.1 ep len 7 avg score 0.4 avg_score_100 0.4 std score 1.2\n",
      "episode  35 score 1.3 ep len 18 avg score 0.4 avg_score_100 0.4 std score 1.2\n",
      "episode  36 score -0.1 ep len 3 avg score 0.4 avg_score_100 0.4 std score 1.2\n",
      "episode  37 score 2.9 ep len 15 avg score 0.4 avg_score_100 0.4 std score 1.2\n",
      "episode  38 score -0.2 ep len 3 avg score 0.4 avg_score_100 0.4 std score 1.2\n",
      "episode  39 score 1.0 ep len 7 avg score 0.4 avg_score_100 0.4 std score 1.2\n",
      "episode  40 score 1.4 ep len 10 avg score 0.5 avg_score_100 0.5 std score 1.2\n",
      "episode  41 score -0.1 ep len 3 avg score 0.5 avg_score_100 0.5 std score 1.2\n",
      "episode  42 score 1.0 ep len 10 avg score 0.5 avg_score_100 0.5 std score 1.2\n",
      "episode  43 score 1.9 ep len 15 avg score 0.5 avg_score_100 0.5 std score 1.2\n",
      "episode  44 score -0.2 ep len 3 avg score 0.5 avg_score_100 0.5 std score 1.2\n",
      "episode  45 score -0.2 ep len 5 avg score 0.5 avg_score_100 0.5 std score 1.2\n",
      "episode  46 score -0.4 ep len 17 avg score 0.5 avg_score_100 0.5 std score 1.2\n",
      "episode  47 score -0.1 ep len 4 avg score 0.4 avg_score_100 0.4 std score 1.1\n",
      "episode  48 score 0.1 ep len 8 avg score 0.4 avg_score_100 0.4 std score 1.1\n",
      "episode  49 score 1.4 ep len 16 avg score 0.5 avg_score_100 0.5 std score 1.1\n",
      "episode  50 score -0.3 ep len 4 avg score 0.4 avg_score_100 0.4 std score 1.1\n",
      "episode  51 score 2.9 ep len 9 avg score 0.5 avg_score_100 0.5 std score 1.2\n",
      "episode  52 score 2.0 ep len 9 avg score 0.5 avg_score_100 0.5 std score 1.2\n",
      "episode  53 score 4.6 ep len 12 avg score 0.6 avg_score_100 0.6 std score 1.3\n",
      "episode  54 score 1.4 ep len 6 avg score 0.6 avg_score_100 0.6 std score 1.3\n",
      "episode  55 score 3.0 ep len 22 avg score 0.6 avg_score_100 0.6 std score 1.3\n",
      "episode  56 score -0.0 ep len 3 avg score 0.6 avg_score_100 0.6 std score 1.3\n",
      "episode  57 score 0.5 ep len 10 avg score 0.6 avg_score_100 0.6 std score 1.3\n",
      "episode  58 score 2.7 ep len 16 avg score 0.7 avg_score_100 0.7 std score 1.3\n",
      "episode  59 score 0.7 ep len 4 avg score 0.7 avg_score_100 0.7 std score 1.3\n",
      "episode  60 score 0.3 ep len 8 avg score 0.7 avg_score_100 0.7 std score 1.3\n",
      "episode  61 score 6.5 ep len 36 avg score 0.8 avg_score_100 0.8 std score 1.5\n",
      "episode  62 score 0.2 ep len 7 avg score 0.7 avg_score_100 0.7 std score 1.5\n",
      "episode  63 score 3.9 ep len 30 avg score 0.8 avg_score_100 0.8 std score 1.5\n",
      "episode  64 score 4.1 ep len 32 avg score 0.8 avg_score_100 0.8 std score 1.5\n",
      "episode  65 score -0.1 ep len 3 avg score 0.8 avg_score_100 0.8 std score 1.5\n",
      "episode  66 score 2.2 ep len 19 avg score 0.9 avg_score_100 0.9 std score 1.5\n",
      "episode  67 score 3.0 ep len 17 avg score 0.9 avg_score_100 0.9 std score 1.5\n",
      "episode  68 score 2.6 ep len 31 avg score 0.9 avg_score_100 0.9 std score 1.6\n",
      "episode  69 score 8.9 ep len 42 avg score 1.0 avg_score_100 1.0 std score 1.8\n",
      "episode  70 score 9.1 ep len 48 avg score 1.1 avg_score_100 1.1 std score 2.0\n",
      "episode  71 score 4.3 ep len 23 avg score 1.2 avg_score_100 1.2 std score 2.1\n",
      "episode  72 score 7.7 ep len 26 avg score 1.3 avg_score_100 1.3 std score 2.2\n",
      "episode  73 score 6.7 ep len 26 avg score 1.3 avg_score_100 1.3 std score 2.2\n",
      "episode  74 score 8.9 ep len 33 avg score 1.4 avg_score_100 1.4 std score 2.4\n",
      "episode  75 score 2.7 ep len 12 avg score 1.5 avg_score_100 1.5 std score 2.4\n",
      "episode  76 score 10.4 ep len 52 avg score 1.6 avg_score_100 1.6 std score 2.6\n",
      "episode  77 score 7.9 ep len 26 avg score 1.7 avg_score_100 1.7 std score 2.7\n",
      "episode  78 score 1.2 ep len 8 avg score 1.7 avg_score_100 1.7 std score 2.6\n",
      "episode  79 score 24.8 ep len 74 avg score 1.9 avg_score_100 1.9 std score 3.7\n",
      "episode  80 score 24.3 ep len 91 avg score 2.2 avg_score_100 2.2 std score 4.4\n",
      "episode  81 score 14.2 ep len 52 avg score 2.4 avg_score_100 2.4 std score 4.6\n",
      "episode  82 score 113.8 ep len 400 avg score 3.7 avg_score_100 3.7 std score 13.0\n",
      "episode  83 score 298.9 ep len 400 avg score 7.2 avg_score_100 7.2 std score 34.5\n",
      "episode  84 score 329.2 ep len 400 avg score 11.0 avg_score_100 11.0 std score 48.8\n",
      "episode  85 score 337.7 ep len 400 avg score 14.8 avg_score_100 14.8 std score 59.8\n",
      "episode  86 score 336.7 ep len 400 avg score 18.5 avg_score_100 18.5 std score 68.7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m     episode_len \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     24\u001b[0m     agent\u001b[38;5;241m.\u001b[39mremember(state\u001b[38;5;241m=\u001b[39mobservation, action\u001b[38;5;241m=\u001b[39maction, done\u001b[38;5;241m=\u001b[39mdone,\n\u001b[1;32m     25\u001b[0m                     reward\u001b[38;5;241m=\u001b[39mreward, new_state\u001b[38;5;241m=\u001b[39mnew_observation)\n\u001b[0;32m---> 26\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     observation \u001b[38;5;241m=\u001b[39m new_observation\n\u001b[1;32m     30\u001b[0m episode_lens\u001b[38;5;241m.\u001b[39mappend(episode_len)\n",
      "File \u001b[0;32m~/Documents/thesis/SAC_reward_Shaped/sac.py:252\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_2\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    248\u001b[0m     critic_2_network_gradient, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_2\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_network_parameters()\n\u001b[0;32m--> 252\u001b[0m \u001b[43mgc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m K\u001b[38;5;241m.\u001b[39mclear_session()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main loop\n",
    "for episode in trange(1000, desc='Test episodes'):\n",
    "\n",
    "        proc.reset()        \n",
    "        (observation, info), done = env.reset(), False\n",
    "        truncated = False\n",
    "        \n",
    "        proc.update_input(observation, info)\n",
    "        observation = proc.get_input()\n",
    "\n",
    "        episode_reward = 0\n",
    "        episode_len = 0\n",
    "\n",
    "        while (not done) and (not truncated):\n",
    "            action = agent.choose_action(observation)\n",
    "\n",
    "            new_observation, reward, done, truncated, new_info = env.step(action=[action])\n",
    "            proc.update_input(new_observation, info)\n",
    "            new_observation = proc.get_input()\n",
    "\n",
    "            episode_reward += reward # type: ignore\n",
    "            episode_len +=1\n",
    "\n",
    "            if new_info[\"rewards\"][\"on_road_reward\"] == False or episode_len >= 200:\n",
    "                truncated = True\n",
    "\n",
    "            agent.remember(state=observation, action=action, done=done,\n",
    "                            reward=reward, new_state=new_observation)\n",
    "            agent.learn()\n",
    "\n",
    "            observation = new_observation\n",
    "\n",
    "        episode_lens.append(episode_len)\n",
    "\n",
    "        score_history.append(episode_reward)\n",
    "        avg_score = np.mean(score_history)\n",
    "        avg_history.append(avg_score)\n",
    "        std_score = np.std(score_history)\n",
    "        std_history.append(std_score)\n",
    "\n",
    "        avg_score_100 = np.mean(score_history[-100:])\n",
    "        avg_history_100.append(avg_score_100)\n",
    "\n",
    "        agent.tensorboard.update_stats(episode_rew = episode_reward, \n",
    "                                       average_rew =avg_score,\n",
    "                                       average_100_reward = avg_score_100,\n",
    "                                       std_rew=std_score, \n",
    "                                       episode_len = episode_len)\n",
    "\n",
    "        print('episode ', episode, 'score %.1f' % episode_reward, 'ep len', episode_len,\n",
    "              'avg score %.1f' % avg_score, 'avg_score_100 %.1f' %avg_score_100,'std score %.1f' % std_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3172\n"
     ]
    }
   ],
   "source": [
    "n_steps = sum(episode_lens)\n",
    "print(n_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/o/Documents/thesis/.venv/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.configure to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.configure` for environment variables or `env.get_wrapper_attr('configure')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/o/Documents/thesis/.venv/lib/python3.10/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/o/Documents/thesis/SAC_reward_Shaped/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae3358e916a4f08b7077649b8e2e7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test episodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/o/Documents/thesis/SAC_reward_Shaped/videos/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/o/Documents/thesis/SAC_reward_Shaped/videos/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/o/Documents/thesis/SAC_reward_Shaped/videos/rl-video-episode-0.mp4\n",
      "episode  0 score 341.1 ep len 400\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# env = record_videos(env)\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "data = load_config()\n",
    "env = gym.make('racetrack-v0', render_mode = 'rgb_array')\n",
    "env.configure(data) # type: ignore\n",
    "\n",
    "env = RecordVideo(env, video_folder=\"videos\", episode_trigger=lambda e: True)\n",
    "env.unwrapped.set_record_video_wrapper(env)\n",
    "\n",
    "\n",
    "for episode in trange(1, desc='Test episodes'):\n",
    "\n",
    "        proc.reset()        \n",
    "        (observation, info), done = env.reset(), False\n",
    "        truncated = False\n",
    "        \n",
    "        proc.update_input(observation, info)\n",
    "        observation = proc.get_input()\n",
    "\n",
    "        episode_reward = 0\n",
    "        episode_len = 0\n",
    "\n",
    "        while (not done) and (not truncated):\n",
    "            action = agent.choose_action(observation)\n",
    "\n",
    "            new_observation, reward, done, truncated, new_info = env.step(action=[action])\n",
    "            proc.update_input(new_observation, info)\n",
    "            new_observation = proc.get_input()\n",
    "\n",
    "            episode_reward += reward # type: ignore\n",
    "            episode_len +=1\n",
    "\n",
    "            observation = new_observation\n",
    "\n",
    "        print('episode ', episode, 'score %.1f' % episode_reward, 'ep len', episode_len)\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
