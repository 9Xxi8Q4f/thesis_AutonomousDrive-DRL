{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fundemental modules\n",
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import pprint\n",
    "from tqdm.notebook import trange\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display visuals \n",
    "from utils import record_videos, show_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning modules\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.utils import plot_model\n",
    "tf.random.set_seed(43)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PER\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    \"\"\"\n",
    "    * init the values\n",
    "    * for DQN actions are discrete\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size, min_size, input_shape, n_actions, discrete=True):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.min_size = min_size\n",
    "        self.discrete = discrete\n",
    "        self.index = 0\n",
    "        \n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape), dtype=np.float16)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape), dtype=np.float16)\n",
    "        dtype = np.int8 if self.discrete else np.float16\n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions), dtype=dtype)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype = np.float16)\n",
    "        self.terminal_memory = np.zeros(self.mem_size)\n",
    "        self.priorities = np.zeros(self.mem_size, dtype=np.float32)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "\n",
    "        index = self.mem_cntr % self.mem_size                \n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "\n",
    "        #* store one hot encoding of actions, if appropriate\n",
    "        if self.discrete:\n",
    "            #* Create an zeros-array size of the number of actions\n",
    "            actions = np.zeros(self.action_memory.shape[1])\n",
    "            #* Make 1 the value of performed action\n",
    "            actions[action] = 1.0\n",
    "            #* Store in action memory\n",
    "            self.action_memory[index] = actions\n",
    "        else:\n",
    "            self.action_memory[index] = action\n",
    "\n",
    "        #* store reward and if it's terminal info \n",
    "        self.reward_memory[index] = reward\n",
    "        #* we send inverse done info!!!\n",
    "        self.terminal_memory[index] = 1 - done\n",
    "        self.priorities[index] = max((self.priorities.max()), 1.0)\n",
    "        self.mem_cntr +=1\n",
    "        self.index = self.mem_cntr\n",
    "\n",
    "    def get_probabilities(self, priority_scale):\n",
    "        scaled_priorities = np.array(self.priorities) ** priority_scale\n",
    "        sample_probabilities = scaled_priorities / sum(scaled_priorities)\n",
    "        return sample_probabilities\n",
    "        \n",
    "    def get_importance(self, probabilities):\n",
    "        importance = 1/(self.mem_cntr) * 1/probabilities\n",
    "        importance_normalized = importance / max(importance)\n",
    "        return importance_normalized\n",
    "\n",
    "    def sample_buffer(self, batch_size, priority_scale=1.0):\n",
    "        \n",
    "        if self.mem_cntr >= self.mem_size:\n",
    "            self.index = self.mem_size\n",
    "            \n",
    "        sample_size = batch_size\n",
    "        sample_probs = self.get_probabilities(priority_scale)\n",
    "        sample_indices = random.choices(range(self.index), k=sample_size, weights=sample_probs[:self.index])\n",
    "\n",
    "        states = self.state_memory[sample_indices]\n",
    "        actions = self.action_memory[sample_indices]\n",
    "        rewards = self.reward_memory[sample_indices]\n",
    "        states_ = self.new_state_memory[sample_indices]\n",
    "        terminal = self.terminal_memory[sample_indices]\n",
    "\n",
    "        # samples = np.array(self.buffer)[sample_indices]\n",
    "        importance = self.get_importance(sample_probs[sample_indices])\n",
    "        return states, actions, rewards, states_, terminal, sample_indices\n",
    "\n",
    "    def set_priorities(self, indices, errors, offset=0.1):\n",
    "        for i,e in zip(indices, errors):\n",
    "            error = abs(e) + offset\n",
    "            clipped_error = np.minimum(error, 1.0)\n",
    "            self.priorities[i] = clipped_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DDQN agent\n",
    "\n",
    "class DDQNAgent:\n",
    "\n",
    "    def __init__(self, alpha, gamma, epsilon, obs_shape,\n",
    "                 batch_size, epsilon_dec, epsilon_end, mem_size, \n",
    "                 min_mem_size, learning_rate, replace_target):\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_dec = epsilon_dec\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_size = mem_size\n",
    "        self.min_mem_size = min_mem_size\n",
    "        self.replace_target = replace_target\n",
    "        self.obs_shape = obs_shape\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.discrete_action_space = np.array([-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "        self.n_actions = len(self.discrete_action_space)\n",
    "        self.action_space = [i for i in range(self.n_actions)]\n",
    "\n",
    "        self.memory = ReplayBuffer(max_size=self.mem_size, min_size=self.min_mem_size,input_shape=self.obs_shape,\n",
    "                             n_actions=self.n_actions,discrete=True)\n",
    "                        \n",
    "        self.q_eval = self._make_model()\n",
    "        self.q_target = self._make_model()      #we keep a target model which we update every K timesteps\n",
    "        self.q_eval.build(input_shape=(None, 128, 64, 4)  )  \n",
    "        self.q_eval.summary()    \n",
    "        plot_model(self.q_eval, to_file='model_ddqn.png')\n",
    "\n",
    "    def _make_model(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, 8, strides=2, activation=\"relu\"))\n",
    "        model.add(Conv2D(64, 4, strides=2, activation=\"relu\"))\n",
    "        model.add(Conv2D(64, 3, strides=1, activation=\"relu\"))\n",
    "        model.add(Flatten())\n",
    "        # model.add( Dense(256, input_dim = self.obs_shape[0], activation='relu') )\n",
    "        model.add( Dense(512, activation='relu') )\n",
    "        model.add( Dense( self.n_actions))\n",
    "        model.compile(loss='mse',optimizer= Adam(learning_rate = self.learning_rate),metrics=[\"accuracy\"]) # type: ignore\n",
    " \n",
    "        return model\n",
    "\n",
    "    def epsilon_decay(self):\n",
    "        self.epsilon = self.epsilon*self.epsilon_dec if self.epsilon > self.epsilon_end \\\n",
    "        else self.epsilon_end\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "\n",
    "    def update_network_parameters(self):\n",
    "        self.q_target.set_weights(self.q_eval.get_weights())\n",
    "        \n",
    "    def get_action(self, observation):\n",
    "\n",
    "        if np.random.random() > self.epsilon: # type: ignore\n",
    "    \n",
    "            # observation = tf.convert_to_tensor(observation, dtype = tf.float16)\n",
    "\n",
    "            qs_= self.q_eval.predict(observation)\n",
    "            action_index = np.argmax(qs_)\n",
    "            action = self.discrete_action_space[action_index]\n",
    "        else:\n",
    "            action_index = np.random.randint(0, self.n_actions)\n",
    "            action = self.discrete_action_space[action_index]\n",
    "        \n",
    "        return action, action_index\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        if (self.memory.mem_cntr) < self.min_mem_size:\n",
    "            return\n",
    "        #* and ELSE:\n",
    "        #* sample minibatch and get states vs..\n",
    "        state, action, reward, new_state, done, sample_indices = \\\n",
    "                            self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "        action_values = np.array(self.action_space, dtype=np.int8)\n",
    "        action_indices = np.dot(action, action_values)\n",
    "\n",
    "        # state = tf.convert_to_tensor(state, dtype = tf.float16)\n",
    "        # new_state = tf.convert_to_tensor(new_state, dtype = tf.float16)\n",
    "        # reward = tf.convert_to_tensor(reward, dtype = tf.float16)\n",
    "        # done = tf.convert_to_tensor(done)\n",
    "        # action_indices = tf.convert_to_tensor(action_indices, dtype=np.int8)\n",
    "        \n",
    "        #* get the q values of current states by main network\n",
    "        q_pred = self.q_eval.predict(state)\n",
    "\n",
    "        #! for abs error\n",
    "        target_old = np.array(q_pred)\n",
    "\n",
    "        #* get the q values of next states by target network\n",
    "        q_next = self.q_target.predict(new_state) #! target_val\n",
    "\n",
    "        #* get the q values of next states by main network\n",
    "        q_eval = self.q_eval.predict(new_state) #! target_next\n",
    "\n",
    "        #* get the actions with highest q values\n",
    "        max_actions = np.argmax(q_eval, axis=1)\n",
    "\n",
    "        #* we will update this dont worry\n",
    "        q_target = q_pred\n",
    "\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "        #* new_q = reward + DISCOUNT * max_future_q\n",
    "        q_target[batch_index, action_indices] = reward + \\\n",
    "                    self.gamma*q_next[batch_index, max_actions.astype(int)]*done\n",
    "\n",
    "        #* error\n",
    "        error = target_old[batch_index, action_indices]-q_target[batch_index, action_indices]\n",
    "        self.memory.set_priorities(sample_indices, error)\n",
    "\n",
    "        #* now we fit the main model (q_eval)\n",
    "        _ = self.q_eval.fit(state, q_target, verbose='auto')\n",
    "\n",
    "        #* If counter reaches set value, update target network with weights of main network\n",
    "        #* it will update it at the very beginning also\n",
    "        if self.memory.mem_cntr & self.replace_target == 0:\n",
    "            self.update_network_parameters()\n",
    "            print(\"Target Updated\")\n",
    "\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "        self.epsilon_decay()\n",
    "\n",
    "    def save_model(self, episode):\n",
    "        print(\"-----saving models------\")\n",
    "        self.q_eval.save_weights(f\"weights/ddqn/q_net-{episode}.h5\")\n",
    "        # self.q_target.save_weights(self.network.checkpoint_file)\n",
    "\n",
    "    def load_model(self):\n",
    "        print(\"-----loading models------\")\n",
    "        self.q_eval.load_weights(\"q_net.h5\")\n",
    "        self.update_network_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observation_shape(observation):\n",
    "    return np.array(np.transpose(observation, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/o/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.configure to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.configure` for environment variables or `env.get_wrapper_attr('configure')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# envirenment config\n",
    "\n",
    "env = gym.make('racetrack-v0', render_mode='rgb_array')\n",
    "env.configure({\n",
    "    'action': {'lateral': True,\n",
    "            'longitudinal': False,\n",
    "            'target_speeds': [0, 5],\n",
    "            'type': 'ContinuousAction'},\n",
    "        \"observation\": {\n",
    "            \"type\": \"GrayscaleObservation\",\n",
    "            \"observation_shape\": (128, 64),\n",
    "            \"stack_size\": 4,\n",
    "            \"weights\": [0.2989, 0.5870, 0.1140],  # weights for RGB conversion\n",
    "            \"scaling\": 1.75,    },\n",
    "    \"other_vehicles\": 1,\n",
    "    'show_trajectories': False,\n",
    "    'collision_reward': -1,\n",
    "    'normalize_reward': True,\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': {'lateral': True,\n",
      "            'longitudinal': False,\n",
      "            'target_speeds': [0, 5],\n",
      "            'type': 'ContinuousAction'},\n",
      " 'action_reward': -0.3,\n",
      " 'centering_position': [0.5, 0.5],\n",
      " 'collision_reward': -1,\n",
      " 'controlled_vehicles': 1,\n",
      " 'duration': 300,\n",
      " 'lane_centering_cost': 4,\n",
      " 'lane_centering_reward': 1,\n",
      " 'manual_control': False,\n",
      " 'normalize_reward': True,\n",
      " 'observation': {'observation_shape': (128, 64),\n",
      "                 'scaling': 1.75,\n",
      "                 'stack_size': 4,\n",
      "                 'type': 'GrayscaleObservation',\n",
      "                 'weights': [0.2989, 0.587, 0.114]},\n",
      " 'offscreen_rendering': False,\n",
      " 'other_vehicles': 1,\n",
      " 'other_vehicles_type': 'highway_env.vehicle.behavior.IDMVehicle',\n",
      " 'policy_frequency': 5,\n",
      " 'real_time_rendering': False,\n",
      " 'render_agent': True,\n",
      " 'scaling': 5.5,\n",
      " 'screen_height': 600,\n",
      " 'screen_width': 600,\n",
      " 'show_trajectories': False,\n",
      " 'simulation_frequency': 15}\n",
      "Environment is setted up.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/o/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.config to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.config` for environment variables or `env.get_wrapper_attr('config')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# prints env configs\n",
    "#* obs is flattened to 1D array for nn\n",
    "\n",
    "pprint.pprint(env.config)\n",
    "(obs, info), done = env.reset(), False\n",
    "obs = observation_shape(obs) \n",
    "print(\"Environment is setted up.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_25 (Conv2D)          (None, 61, 29, 32)        8224      \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, 29, 13, 64)        32832     \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          (None, 27, 11, 64)        36928     \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 19008)             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 512)               9732608   \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 9)                 4617      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,815,209\n",
      "Trainable params: 9,815,209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Agent is initialized.\n"
     ]
    }
   ],
   "source": [
    "# agent config\n",
    "\n",
    "agent = DDQNAgent(alpha=0.001, gamma=0.9, epsilon=1.0, obs_shape=obs.shape,\n",
    "                  batch_size=64, epsilon_dec=0.999, epsilon_end=0.05, mem_size=20000,\n",
    "                  min_mem_size=100, replace_target=100, learning_rate=0.001)\n",
    "\n",
    "print(\"Agent is initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* basic params for training\n",
    "\n",
    "best_score = -1000.0\n",
    "score_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main loop\n",
    "\n",
    "#TODO-1: Parameters Tuning\n",
    "\n",
    "#TODO-6: Actions Shaping\n",
    "\n",
    "#TODO-7: Pruning\n",
    "\n",
    "#TODO-8: Tensorboard\n",
    "\n",
    "env = record_videos(env)\n",
    "\n",
    "for episode in trange(2000, desc='Test episodes'):\n",
    "        (observation, info), done = env.reset(), False\n",
    "        observation = observation_shape(observation)\n",
    "\n",
    "\n",
    "        done_ = False\n",
    "        score = 0\n",
    "        step = 0\n",
    "        truncated = False\n",
    "        # env.render()\n",
    "        while not done_:\n",
    "            action, action_index = agent.get_action(observation)\n",
    "            new_observation, reward, done, truncated, info = env.step(action=[action])\n",
    "            new_observation = observation_shape(new_observation)\n",
    "\n",
    "            if info[\"crashed\"] == True or info[\"rewards\"][\"on_road_reward\"] == False or truncated == True:\n",
    "                done_ = True\n",
    "                reward = -1.0\n",
    "            else: done_ = False\n",
    "\n",
    "            score += reward\n",
    "\n",
    "            agent.remember(state=observation, action=action_index, done=done_,\n",
    "                            reward=reward, new_state=new_observation)\n",
    "            agent.train()\n",
    "\n",
    "            observation = new_observation\n",
    "            \n",
    "\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            agent.save_model(episode)\n",
    "\n",
    "        print('episode ', episode, 'score %.1f' % score,\n",
    "               'avg score %.1f' % avg_score)\n",
    "        print(\"Exp- value:\", agent.epsilon)\n",
    "        time.sleep(1)\n",
    "\n",
    "env.close()\n",
    "# show_videos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.save(\"score_history\", np.array(score_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_score)\n",
    "print(avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating\n",
    "agent.load_model()\n",
    "\n",
    "env = record_videos(env)\n",
    "\n",
    "for episode in trange(10, desc='Test episodes'):\n",
    "        (observation, info), done = env.reset(), False\n",
    "        observation = np.array(observation.flatten())\n",
    "\n",
    "        done_ = False\n",
    "        score = 0\n",
    "        step = 0\n",
    "        # env.render()\n",
    "        while True:\n",
    "            action, action_index = agent.get_action(observation.reshape((1,observation.shape[0])))\n",
    "            new_observation, reward, done, truncated, info = env.step(action=[action])\n",
    "            new_observation = np.array(new_observation.flatten())\n",
    "\n",
    "            if info[\"crashed\"] == True or info[\"rewards\"][\"on_road_reward\"] == False:\n",
    "                done_ = True\n",
    "                reward = -1.0\n",
    "            else: done_ = False\n",
    "\n",
    "            score += reward\n",
    "\n",
    "\n",
    "            observation = new_observation\n",
    "\n",
    "            if done or done_:\n",
    "                break\n",
    "\n",
    "\n",
    "        print(\"Episode: \", episode)\n",
    "        print(\"Score: \", score)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
