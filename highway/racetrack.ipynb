{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fundemental modules\n",
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import pprint\n",
    "from tqdm.notebook import trange\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display visuals \n",
    "from utils import record_videos, show_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 22:59:13.039979: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-11 22:59:13.229146: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-11 22:59:13.266354: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-10-11 22:59:13.266381: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-10-11 22:59:14.010364: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-11 22:59:14.010399: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-11 22:59:14.010402: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# deep learning modules\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.utils import plot_model\n",
    "tf.random.set_seed(43)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PER\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    \"\"\"\n",
    "    * init the values\n",
    "    * for DQN actions are discrete\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size, min_size, input_shape, n_actions, discrete=True):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.min_size = min_size\n",
    "        self.discrete = discrete\n",
    "        self.index = 0\n",
    "        \n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape), dtype=np.float16)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape), dtype=np.float16)\n",
    "        dtype = np.int8 if self.discrete else np.float16\n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions), dtype=dtype)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype = np.float16)\n",
    "        self.terminal_memory = np.zeros(self.mem_size)\n",
    "        self.priorities = np.zeros(self.mem_size, dtype=np.float32)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "\n",
    "        index = self.mem_cntr % self.mem_size                \n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "\n",
    "        #* store one hot encoding of actions, if appropriate\n",
    "        if self.discrete:\n",
    "            #* Create an zeros-array size of the number of actions\n",
    "            actions = np.zeros(self.action_memory.shape[1])\n",
    "            #* Make 1 the value of performed action\n",
    "            actions[action] = 1.0\n",
    "            #* Store in action memory\n",
    "            self.action_memory[index] = actions\n",
    "        else:\n",
    "            self.action_memory[index] = action\n",
    "\n",
    "        #* store reward and if it's terminal info \n",
    "        self.reward_memory[index] = reward\n",
    "        #* we send inverse done info!!!\n",
    "        self.terminal_memory[index] = 1 - done\n",
    "        self.priorities[index] = max((self.priorities.max()), 1.0)\n",
    "        self.mem_cntr +=1\n",
    "        self.index = self.mem_cntr\n",
    "\n",
    "    def get_probabilities(self, priority_scale):\n",
    "        scaled_priorities = np.array(self.priorities) ** priority_scale\n",
    "        sample_probabilities = scaled_priorities / sum(scaled_priorities)\n",
    "        return sample_probabilities\n",
    "        \n",
    "    def get_importance(self, probabilities):\n",
    "        importance = 1/(self.mem_cntr) * 1/probabilities\n",
    "        importance_normalized = importance / max(importance)\n",
    "        return importance_normalized\n",
    "\n",
    "    def sample_buffer(self, batch_size, priority_scale=1.0):\n",
    "        \n",
    "        if self.mem_cntr >= self.mem_size:\n",
    "            self.index = self.mem_size\n",
    "            \n",
    "        sample_size = batch_size\n",
    "        sample_probs = self.get_probabilities(priority_scale)\n",
    "        sample_indices = random.choices(range(self.index), k=sample_size, weights=sample_probs[:self.index])\n",
    "\n",
    "        states = self.state_memory[sample_indices]\n",
    "        actions = self.action_memory[sample_indices]\n",
    "        rewards = self.reward_memory[sample_indices]\n",
    "        states_ = self.new_state_memory[sample_indices]\n",
    "        terminal = self.terminal_memory[sample_indices]\n",
    "\n",
    "        # samples = np.array(self.buffer)[sample_indices]\n",
    "        importance = self.get_importance(sample_probs[sample_indices])\n",
    "        return states, actions, rewards, states_, terminal, sample_indices\n",
    "\n",
    "    def set_priorities(self, indices, errors, offset=0.1):\n",
    "        for i,e in zip(indices, errors):\n",
    "            error = abs(e) + offset\n",
    "            clipped_error = np.minimum(error, 1.0)\n",
    "            self.priorities[i] = clipped_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DDQN agent\n",
    "\n",
    "class DDQNAgent:\n",
    "\n",
    "    def __init__(self, alpha, gamma, epsilon, obs_shape,\n",
    "                 batch_size, epsilon_dec, epsilon_end, mem_size, \n",
    "                 min_mem_size, learning_rate, replace_target):\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_dec = epsilon_dec\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_size = mem_size\n",
    "        self.min_mem_size = min_mem_size\n",
    "        self.replace_target = replace_target\n",
    "        self.obs_shape = obs_shape\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.discrete_action_space = np.array([-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "        self.n_actions = len(self.discrete_action_space)\n",
    "        self.action_space = [i for i in range(self.n_actions)]\n",
    "\n",
    "        self.memory = ReplayBuffer(max_size=self.mem_size, min_size=self.min_mem_size,input_shape=self.obs_shape,\n",
    "                             n_actions=self.n_actions,discrete=True)\n",
    "                        \n",
    "        self.q_eval = self._make_model()\n",
    "        self.q_target = self._make_model()      #we keep a target model which we update every K timesteps\n",
    "        self.q_eval.summary()\n",
    "        plot_model(self.q_eval, to_file='./model_ddqn.png')\n",
    "\n",
    "    def _make_model(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add( Dense(256, input_dim = self.obs_shape[0], activation='relu') )\n",
    "        model.add( Dense(256, activation='relu') )\n",
    "        model.add( Dense( self.n_actions))\n",
    "        model.compile(loss='mse',optimizer= Adam(learning_rate = self.learning_rate),metrics=[\"accuracy\"]) # type: ignore\n",
    " \n",
    "        return model\n",
    "\n",
    "    def epsilon_decay(self):\n",
    "        self.epsilon = self.epsilon*self.epsilon_dec if self.epsilon > self.epsilon_end \\\n",
    "        else self.epsilon_end\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "\n",
    "    def update_network_parameters(self):\n",
    "        self.q_target.set_weights(self.q_eval.get_weights())\n",
    "        \n",
    "    def get_action(self, observation):\n",
    "\n",
    "        if np.random.random() > self.epsilon: # type: ignore\n",
    "    \n",
    "            # observation = tf.convert_to_tensor(observation, dtype = tf.float16)\n",
    "\n",
    "            qs_= self.q_eval.predict(observation)\n",
    "            action_index = np.argmax(qs_)\n",
    "            action = self.discrete_action_space[action_index]\n",
    "        else:\n",
    "            action_index = np.random.randint(0, self.n_actions)\n",
    "            action = self.discrete_action_space[action_index]\n",
    "        \n",
    "        return action, action_index\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        if (self.memory.mem_cntr) < self.min_mem_size:\n",
    "            return\n",
    "        #* and ELSE:\n",
    "        #* sample minibatch and get states vs..\n",
    "        state, action, reward, new_state, done, sample_indices = \\\n",
    "                            self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "        action_values = np.array(self.action_space, dtype=np.int8)\n",
    "        action_indices = np.dot(action, action_values)\n",
    "\n",
    "        # state = tf.convert_to_tensor(state, dtype = tf.float16)\n",
    "        # new_state = tf.convert_to_tensor(new_state, dtype = tf.float16)\n",
    "        # reward = tf.convert_to_tensor(reward, dtype = tf.float16)\n",
    "        # done = tf.convert_to_tensor(done)\n",
    "        # action_indices = tf.convert_to_tensor(action_indices, dtype=np.int8)\n",
    "        \n",
    "        #* get the q values of current states by main network\n",
    "        q_pred = self.q_eval.predict(state)\n",
    "\n",
    "        #! for abs error\n",
    "        target_old = np.array(q_pred)\n",
    "\n",
    "        #* get the q values of next states by target network\n",
    "        q_next = self.q_target.predict(new_state) #! target_val\n",
    "\n",
    "        #* get the q values of next states by main network\n",
    "        q_eval = self.q_eval.predict(new_state) #! target_next\n",
    "\n",
    "        #* get the actions with highest q values\n",
    "        max_actions = np.argmax(q_eval, axis=1)\n",
    "\n",
    "        #* we will update this dont worry\n",
    "        q_target = q_pred\n",
    "\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "        #* new_q = reward + DISCOUNT * max_future_q\n",
    "        q_target[batch_index, action_indices] = reward + \\\n",
    "                    self.gamma*q_next[batch_index, max_actions.astype(int)]*done\n",
    "\n",
    "        #* error\n",
    "        error = target_old[batch_index, action_indices]-q_target[batch_index, action_indices]\n",
    "        self.memory.set_priorities(sample_indices, error)\n",
    "\n",
    "        #* now we fit the main model (q_eval)\n",
    "        _ = self.q_eval.fit(state, q_target, verbose='auto')\n",
    "\n",
    "        #* If counter reaches set value, update target network with weights of main network\n",
    "        #* it will update it at the very beginning also\n",
    "        if self.memory.mem_cntr & self.replace_target == 0:\n",
    "            self.update_network_parameters()\n",
    "            print(\"Target Updated\")\n",
    "\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "        self.epsilon_decay()\n",
    "\n",
    "    def save_model(self, episode):\n",
    "        print(\"-----saving models------\")\n",
    "        self.q_eval.save_weights(f\"weights/ddqn/q_net-{episode}.h5\")\n",
    "        # self.q_target.save_weights(self.network.checkpoint_file)\n",
    "\n",
    "    def load_model(self):\n",
    "        print(\"-----loading models------\")\n",
    "        self.q_eval.load_weights(\"q_net.h5\")\n",
    "        self.update_network_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# envirenment config\n",
    "\n",
    "env = gym.make('racetrack-v0', render_mode='rgb_array')\n",
    "env.configure({\n",
    "    'action': {'lateral': True,\n",
    "            'longitudinal': False,\n",
    "            'target_speeds': [0, 5],\n",
    "            'type': 'ContinuousAction'},\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\",\n",
    "        \"vehicles_count\": 1,\n",
    "        \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\", \"cos_h\", \"sin_h\",\n",
    "                     \"heading\", \"long_off\", \"lat_off\", \"ang_off\"],\n",
    "    },\n",
    "    'show_trajectories': True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': {'lateral': True,\n",
      "            'longitudinal': False,\n",
      "            'target_speeds': [0, 5],\n",
      "            'type': 'ContinuousAction'},\n",
      " 'action_reward': -0.3,\n",
      " 'centering_position': [0.5, 0.5],\n",
      " 'collision_reward': -1,\n",
      " 'controlled_vehicles': 1,\n",
      " 'duration': 300,\n",
      " 'lane_centering_cost': 4,\n",
      " 'lane_centering_reward': 1,\n",
      " 'manual_control': False,\n",
      " 'observation': {'features': ['presence',\n",
      "                              'x',\n",
      "                              'y',\n",
      "                              'vx',\n",
      "                              'vy',\n",
      "                              'cos_h',\n",
      "                              'sin_h',\n",
      "                              'heading',\n",
      "                              'long_off',\n",
      "                              'lat_off',\n",
      "                              'ang_off'],\n",
      "                 'type': 'Kinematics',\n",
      "                 'vehicles_count': 1},\n",
      " 'offscreen_rendering': False,\n",
      " 'other_vehicles': 1,\n",
      " 'other_vehicles_type': 'highway_env.vehicle.behavior.IDMVehicle',\n",
      " 'policy_frequency': 5,\n",
      " 'real_time_rendering': False,\n",
      " 'render_agent': True,\n",
      " 'scaling': 5.5,\n",
      " 'screen_height': 600,\n",
      " 'screen_width': 600,\n",
      " 'show_trajectories': True,\n",
      " 'simulation_frequency': 15}\n",
      "Environment is setted up.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/o/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    }
   ],
   "source": [
    "# prints env configs\n",
    "#* obs is flattened to 1D array for nn\n",
    "\n",
    "pprint.pprint(env.config)\n",
    "(obs, info), done = env.reset(), False\n",
    "obs = np.array(obs.flatten())\n",
    "print(\"Environment is setted up.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 256)               5888      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 9)                 2313      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 73,993\n",
      "Trainable params: 73,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Agent is initialized.\n"
     ]
    }
   ],
   "source": [
    "# agent config\n",
    "\n",
    "agent = DDQNAgent(alpha=0.001, gamma=0.9, epsilon=1.0, obs_shape=obs.shape,\n",
    "                  batch_size=64, epsilon_dec=0.999, epsilon_end=0.05, mem_size=20000,\n",
    "                  min_mem_size=100, replace_target=100, learning_rate=0.001)\n",
    "\n",
    "print(\"Agent is initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* basic params for training\n",
    "\n",
    "best_score = -1000.0\n",
    "score_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/o/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/o/Documents/thesis/highway/videos/trainings/ddqn folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de6959d2dff42fe915477d82b817725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test episodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4893 - accuracy: 0.7500\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0223 - accuracy: 0.6875\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.9015 - accuracy: 0.6875\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5871 - accuracy: 0.7031\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.8571 - accuracy: 0.7812\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.8166 - accuracy: 0.7188\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.9814 - accuracy: 0.7500\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.8756 - accuracy: 0.8281\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3573 - accuracy: 0.8125\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0039 - accuracy: 0.8125\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7923 - accuracy: 0.8594\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.9865 - accuracy: 0.7500\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3695 - accuracy: 0.7656\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2343 - accuracy: 0.8594\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4877 - accuracy: 0.8594\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6784 - accuracy: 0.8125\n",
      "episode  0 score 7.8 avg score 4.9\n",
      "Exp- value: 0.6261068997312542\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0539 - accuracy: 0.8438\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7512 - accuracy: 0.8281\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.4774 - accuracy: 0.8125\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4783 - accuracy: 0.9062\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4617 - accuracy: 0.8906\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6779 - accuracy: 0.8750\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7011 - accuracy: 0.8438\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6504 - accuracy: 0.7188\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6621 - accuracy: 0.7656\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5940 - accuracy: 0.8906\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1442 - accuracy: 0.8906\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0437 - accuracy: 0.9219\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.8820 - accuracy: 0.8906\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.9452 - accuracy: 0.8438\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.9514 - accuracy: 0.7812\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2918 - accuracy: 0.7812\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7503 - accuracy: 0.8125\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6092 - accuracy: 0.7656\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1105 - accuracy: 0.7812\n",
      "episode  1 score 8.0 avg score 4.9\n",
      "Exp- value: 0.6143173286381555\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.9733 - accuracy: 0.7812\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6295 - accuracy: 0.7188\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6869 - accuracy: 0.8750\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7657 - accuracy: 0.8750\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6791 - accuracy: 0.7969\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.8510 - accuracy: 0.8594\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5662 - accuracy: 0.7344\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5321 - accuracy: 0.8750\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5180 - accuracy: 0.8750\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7217 - accuracy: 0.8594\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5736 - accuracy: 0.7969\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6167 - accuracy: 0.7188\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.8617 - accuracy: 0.7656\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6007 - accuracy: 0.8125\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5153 - accuracy: 0.7812\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6740 - accuracy: 0.8594\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5614 - accuracy: 0.8438\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.3998 - accuracy: 0.8594\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0920 - accuracy: 0.7812\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5845 - accuracy: 0.7969\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7211 - accuracy: 0.7656\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5053 - accuracy: 0.7656\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.9364 - accuracy: 0.8281\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5117 - accuracy: 0.7656\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5945 - accuracy: 0.7969\n",
      "episode  2 score 13.1 avg score 5.1\n",
      "Exp- value: 0.5991422854295215\n",
      "Moviepy - Building video /home/o/Documents/thesis/highway/videos/trainings/ddqn/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/o/Documents/thesis/highway/videos/trainings/ddqn/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/o/Documents/thesis/highway/videos/trainings/ddqn/rl-video-episode-0.mp4\n",
      "Moviepy - Building video /home/o/Documents/thesis/highway/videos/trainings/ddqn/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/o/Documents/thesis/highway/videos/trainings/ddqn/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/o/Documents/thesis/highway/videos/trainings/ddqn/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# main loop\n",
    "\n",
    "#TODO-1: Parameters Tuning\n",
    "\n",
    "#TODO-6: Actions Shaping\n",
    "\n",
    "#TODO-7: Pruning\n",
    "\n",
    "#TODO-8: Tensorboard\n",
    "\n",
    "env = record_videos(env)\n",
    "\n",
    "for episode in trange(1000, desc='Test episodes'):\n",
    "        (observation, info), done = env.reset(), False\n",
    "        observation = np.array(observation.flatten())\n",
    "\n",
    "\n",
    "        done_ = False\n",
    "        score = 0\n",
    "        step = 0\n",
    "        truncated = False\n",
    "        # env.render()\n",
    "        while not done_:\n",
    "            action, action_index = agent.get_action(observation.reshape((1,observation.shape[0])))\n",
    "            new_observation, reward, done, truncated, info = env.step(action=[action])\n",
    "            new_observation = np.array(new_observation.flatten())\n",
    "\n",
    "            if info[\"crashed\"] == True or info[\"rewards\"][\"on_road_reward\"] == False or truncated == True:\n",
    "                done_ = True\n",
    "                reward = -10.0\n",
    "            else: done_ = False\n",
    "\n",
    "            score += reward\n",
    "\n",
    "            agent.remember(state=observation, action=action_index, done=done_,\n",
    "                            reward=reward, new_state=new_observation)\n",
    "            agent.train()\n",
    "\n",
    "            observation = new_observation\n",
    "            \n",
    "\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            agent.save_model(episode)\n",
    "\n",
    "        print('episode ', episode, 'score %.1f' % score,\n",
    "               'avg score %.1f' % avg_score)\n",
    "        print(\"Exp- value:\", agent.epsilon)\n",
    "        time.sleep(1)\n",
    "\n",
    "env.close()\n",
    "# show_videos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.save(\"score_history\", np.array(score_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.02133133098047\n",
      "1.974266679505516\n"
     ]
    }
   ],
   "source": [
    "print(best_score)\n",
    "print(avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----loading models------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/o/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/o/Documents/thesis/highway/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05260480bc24a238c5aabf9662e532a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test episodes:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0\n",
      "Score:  1.3721311968406065\n",
      "Episode:  1\n",
      "Score:  5.0530271586894075\n",
      "Episode:  2\n",
      "Score:  6.577469628791425\n",
      "Episode:  3\n",
      "Score:  -0.05288229273136713\n",
      "Episode:  4\n",
      "Score:  0.775702045658262\n",
      "Episode:  5\n",
      "Score:  4.964922426753592\n",
      "Episode:  6\n",
      "Score:  9.828439108016884\n",
      "Episode:  7\n",
      "Score:  5.319425406038121\n",
      "Episode:  8\n",
      "Score:  2.6945582932489436\n",
      "Episode:  9\n",
      "Score:  1.8040322219631442\n",
      "Moviepy - Building video /home/o/Documents/thesis/highway/videos/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/o/Documents/thesis/highway/videos/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/o/Documents/thesis/highway/videos/rl-video-episode-0.mp4\n",
      "Moviepy - Building video /home/o/Documents/thesis/highway/videos/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/o/Documents/thesis/highway/videos/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/o/Documents/thesis/highway/videos/rl-video-episode-0.mp4\n",
      "Moviepy - Building video /home/o/Documents/thesis/highway/videos/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/o/Documents/thesis/highway/videos/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/o/Documents/thesis/highway/videos/rl-video-episode-0.mp4\n",
      "Moviepy - Building video /home/o/Documents/thesis/highway/videos/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/o/Documents/thesis/highway/videos/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/o/Documents/thesis/highway/videos/rl-video-episode-0.mp4\n"
     ]
    }
   ],
   "source": [
    "#Evaluating\n",
    "agent.load_model()\n",
    "\n",
    "env = record_videos(env)\n",
    "\n",
    "for episode in trange(10, desc='Test episodes'):\n",
    "        (observation, info), done = env.reset(), False\n",
    "        observation = np.array(observation.flatten())\n",
    "\n",
    "        done_ = False\n",
    "        score = 0\n",
    "        step = 0\n",
    "        # env.render()\n",
    "        while True:\n",
    "            action, action_index = agent.get_action(observation.reshape((1,observation.shape[0])))\n",
    "            new_observation, reward, done, truncated, info = env.step(action=[action])\n",
    "            new_observation = np.array(new_observation.flatten())\n",
    "\n",
    "            if info[\"crashed\"] == True or info[\"rewards\"][\"on_road_reward\"] == False:\n",
    "                done_ = True\n",
    "                reward = -1.0\n",
    "            else: done_ = False\n",
    "\n",
    "            score += reward\n",
    "\n",
    "\n",
    "            observation = new_observation\n",
    "\n",
    "            if done or done_:\n",
    "                break\n",
    "\n",
    "\n",
    "        print(\"Episode: \", episode)\n",
    "        print(\"Score: \", score)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
