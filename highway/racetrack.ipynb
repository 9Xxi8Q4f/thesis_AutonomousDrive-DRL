{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fundemental modules\n",
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import pprint\n",
    "from tqdm.notebook import trange\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display visuals \n",
    "from utils import record_videos, show_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 22:00:34.845676: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-10 22:00:35.029148: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-10 22:00:35.066941: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-10-10 22:00:35.066969: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-10-10 22:00:35.807757: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-10 22:00:35.807792: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-10 22:00:35.807795: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# deep learning modules\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.utils import plot_model\n",
    "tf.random.set_seed(43)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PER\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    \"\"\"\n",
    "    * init the values\n",
    "    * for DQN actions are discrete\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size, min_size, input_shape, n_actions, discrete=True):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.min_size = min_size\n",
    "        self.discrete = discrete\n",
    "        self.index = 0\n",
    "        \n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape), dtype=np.float16)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape), dtype=np.float16)\n",
    "        dtype = np.int8 if self.discrete else np.float16\n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions), dtype=dtype)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype = np.float16)\n",
    "        self.terminal_memory = np.zeros(self.mem_size)\n",
    "        self.priorities = np.zeros(self.mem_size, dtype=np.float32)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "\n",
    "        index = self.mem_cntr % self.mem_size                \n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "\n",
    "        #* store one hot encoding of actions, if appropriate\n",
    "        if self.discrete:\n",
    "            #* Create an zeros-array size of the number of actions\n",
    "            actions = np.zeros(self.action_memory.shape[1])\n",
    "            #* Make 1 the value of performed action\n",
    "            actions[action] = 1.0\n",
    "            #* Store in action memory\n",
    "            self.action_memory[index] = actions\n",
    "        else:\n",
    "            self.action_memory[index] = action\n",
    "\n",
    "        #* store reward and if it's terminal info \n",
    "        self.reward_memory[index] = reward\n",
    "        #* we send inverse done info!!!\n",
    "        self.terminal_memory[index] = 1 - done\n",
    "        self.priorities[index] = max((self.priorities.max()), 1.0)\n",
    "        self.mem_cntr +=1\n",
    "        self.index = self.mem_cntr\n",
    "\n",
    "    def get_probabilities(self, priority_scale):\n",
    "        scaled_priorities = np.array(self.priorities) ** priority_scale\n",
    "        sample_probabilities = scaled_priorities / sum(scaled_priorities)\n",
    "        return sample_probabilities\n",
    "        \n",
    "    def get_importance(self, probabilities):\n",
    "        importance = 1/(self.mem_cntr) * 1/probabilities\n",
    "        importance_normalized = importance / max(importance)\n",
    "        return importance_normalized\n",
    "\n",
    "    def sample_buffer(self, batch_size, priority_scale=1.0):\n",
    "        \n",
    "        if self.mem_cntr >= self.mem_size:\n",
    "            self.index = self.mem_size\n",
    "            \n",
    "        sample_size = batch_size\n",
    "        sample_probs = self.get_probabilities(priority_scale)\n",
    "        sample_indices = random.choices(range(self.index), k=sample_size, weights=sample_probs[:self.index])\n",
    "\n",
    "        states = self.state_memory[sample_indices]\n",
    "        actions = self.action_memory[sample_indices]\n",
    "        rewards = self.reward_memory[sample_indices]\n",
    "        states_ = self.new_state_memory[sample_indices]\n",
    "        terminal = self.terminal_memory[sample_indices]\n",
    "\n",
    "        # samples = np.array(self.buffer)[sample_indices]\n",
    "        importance = self.get_importance(sample_probs[sample_indices])\n",
    "        return states, actions, rewards, states_, terminal, sample_indices\n",
    "\n",
    "    def set_priorities(self, indices, errors, offset=0.1):\n",
    "        for i,e in zip(indices, errors):\n",
    "            error = abs(e) + offset\n",
    "            clipped_error = np.minimum(error, 1.0)\n",
    "            self.priorities[i] = clipped_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DDQN agent\n",
    "\n",
    "class DDQNAgent:\n",
    "\n",
    "    def __init__(self, alpha, gamma, epsilon, obs_shape,\n",
    "                 batch_size, epsilon_dec, epsilon_end, mem_size, \n",
    "                 min_mem_size, learning_rate, replace_target):\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_dec = epsilon_dec\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_size = mem_size\n",
    "        self.min_mem_size = min_mem_size\n",
    "        self.replace_target = replace_target\n",
    "        self.obs_shape = obs_shape\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.discrete_action_space = np.array([-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "        self.n_actions = len(self.discrete_action_space)\n",
    "        self.action_space = [i for i in range(self.n_actions)]\n",
    "\n",
    "        self.memory = ReplayBuffer(max_size=self.mem_size, min_size=self.min_mem_size,input_shape=self.obs_shape,\n",
    "                             n_actions=self.n_actions,discrete=True)\n",
    "                        \n",
    "        self.q_eval = self._make_model()\n",
    "        self.q_target = self._make_model()      #we keep a target model which we update every K timesteps\n",
    "        self.q_eval.summary()\n",
    "        plot_model(self.q_eval, to_file='./model_ddqn.png')\n",
    "\n",
    "    def _make_model(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add( Dense(256, input_dim = self.obs_shape[0], activation='relu') )\n",
    "        model.add( Dense(256, activation='relu') )\n",
    "        model.add( Dense( self.n_actions))\n",
    "        model.compile(loss='mse',optimizer= Adam(learning_rate = self.learning_rate),metrics=[\"accuracy\"]) # type: ignore\n",
    " \n",
    "        return model\n",
    "\n",
    "    def epsilon_decay(self):\n",
    "        self.epsilon = self.epsilon*self.epsilon_dec if self.epsilon > self.epsilon_end \\\n",
    "        else self.epsilon_end\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "\n",
    "    def update_network_parameters(self):\n",
    "        self.q_target.set_weights(self.q_eval.get_weights())\n",
    "        \n",
    "    def get_action(self, observation):\n",
    "\n",
    "        if np.random.random() > self.epsilon: # type: ignore\n",
    "    \n",
    "            # observation = tf.convert_to_tensor(observation, dtype = tf.float16)\n",
    "\n",
    "            qs_= self.q_eval.predict(observation)\n",
    "            action_index = np.argmax(qs_)\n",
    "            action = self.discrete_action_space[action_index]\n",
    "        else:\n",
    "            action_index = np.random.randint(0, self.n_actions)\n",
    "            action = self.discrete_action_space[action_index]\n",
    "        \n",
    "        return action, action_index\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        if (self.memory.mem_cntr) < self.min_mem_size:\n",
    "            return\n",
    "        #* and ELSE:\n",
    "        #* sample minibatch and get states vs..\n",
    "        state, action, reward, new_state, done, sample_indices = \\\n",
    "                            self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "        action_values = np.array(self.action_space, dtype=np.int8)\n",
    "        action_indices = np.dot(action, action_values)\n",
    "\n",
    "        # state = tf.convert_to_tensor(state, dtype = tf.float16)\n",
    "        # new_state = tf.convert_to_tensor(new_state, dtype = tf.float16)\n",
    "        # reward = tf.convert_to_tensor(reward, dtype = tf.float16)\n",
    "        # done = tf.convert_to_tensor(done)\n",
    "        # action_indices = tf.convert_to_tensor(action_indices, dtype=np.int8)\n",
    "        \n",
    "        #* get the q values of current states by main network\n",
    "        q_pred = self.q_eval.predict(state)\n",
    "\n",
    "        #! for abs error\n",
    "        target_old = np.array(q_pred)\n",
    "\n",
    "        #* get the q values of next states by target network\n",
    "        q_next = self.q_target.predict(new_state) #! target_val\n",
    "\n",
    "        #* get the q values of next states by main network\n",
    "        q_eval = self.q_eval.predict(new_state) #! target_next\n",
    "\n",
    "        #* get the actions with highest q values\n",
    "        max_actions = np.argmax(q_eval, axis=1)\n",
    "\n",
    "        #* we will update this dont worry\n",
    "        q_target = q_pred\n",
    "\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "        #* new_q = reward + DISCOUNT * max_future_q\n",
    "        q_target[batch_index, action_indices] = reward + \\\n",
    "                    self.gamma*q_next[batch_index, max_actions.astype(int)]*done\n",
    "\n",
    "        #* error\n",
    "        error = target_old[batch_index, action_indices]-q_target[batch_index, action_indices]\n",
    "        self.memory.set_priorities(sample_indices, error)\n",
    "\n",
    "        #* now we fit the main model (q_eval)\n",
    "        _ = self.q_eval.fit(state, q_target, verbose='auto')\n",
    "\n",
    "        #* If counter reaches set value, update target network with weights of main network\n",
    "        #* it will update it at the very beginning also\n",
    "        if self.memory.mem_cntr & self.replace_target == 0:\n",
    "            self.update_network_parameters()\n",
    "            print(\"Target Updated\")\n",
    "\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "        self.epsilon_decay()\n",
    "\n",
    "    def save_model(self, episode):\n",
    "        print(\"-----saving models------\")\n",
    "        self.q_eval.save_weights(f\"weights/ddqn/q_net-{episode}.h5\")\n",
    "        # self.q_target.save_weights(self.network.checkpoint_file)\n",
    "\n",
    "    def load_model(self):\n",
    "        print(\"-----loading models------\")\n",
    "        self.q_eval.load_weights(\"q_net.h5\")\n",
    "        self.update_network_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/o/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.configure to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.configure` for environment variables or `env.get_wrapper_attr('configure')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# envirenment config\n",
    "\n",
    "env = gym.make('racetrack-v0', render_mode='rgb_array')\n",
    "env.configure({\n",
    "    'action': {'lateral': True,\n",
    "            'longitudinal': False,\n",
    "            'target_speeds': [0, 5],\n",
    "            'type': 'ContinuousAction'},\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\",\n",
    "        \"vehicles_count\": 2,\n",
    "        \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\", \"cos_h\", \"sin_h\",\n",
    "                     \"heading\", \"long_off\", \"lat_off\", \"ang_off\"],\n",
    "    },\n",
    "    \"other_vehicles\": 1,\n",
    "    'show_trajectories': True,\n",
    "     'offroad_terminal': True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': {'lateral': True,\n",
      "            'longitudinal': False,\n",
      "            'target_speeds': [0, 5],\n",
      "            'type': 'ContinuousAction'},\n",
      " 'action_reward': -0.3,\n",
      " 'centering_position': [0.5, 0.5],\n",
      " 'collision_reward': -1,\n",
      " 'controlled_vehicles': 1,\n",
      " 'duration': 300,\n",
      " 'lane_centering_cost': 4,\n",
      " 'lane_centering_reward': 1,\n",
      " 'manual_control': False,\n",
      " 'observation': {'features': ['presence',\n",
      "                              'x',\n",
      "                              'y',\n",
      "                              'vx',\n",
      "                              'vy',\n",
      "                              'cos_h',\n",
      "                              'sin_h',\n",
      "                              'heading',\n",
      "                              'long_off',\n",
      "                              'lat_off',\n",
      "                              'ang_off'],\n",
      "                 'type': 'Kinematics',\n",
      "                 'vehicles_count': 2},\n",
      " 'offroad_terminal': True,\n",
      " 'offscreen_rendering': False,\n",
      " 'other_vehicles': 1,\n",
      " 'other_vehicles_type': 'highway_env.vehicle.behavior.IDMVehicle',\n",
      " 'policy_frequency': 5,\n",
      " 'real_time_rendering': False,\n",
      " 'render_agent': True,\n",
      " 'scaling': 5.5,\n",
      " 'screen_height': 600,\n",
      " 'screen_width': 600,\n",
      " 'show_trajectories': True,\n",
      " 'simulation_frequency': 15}\n",
      "Environment is setted up.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/o/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.config to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.config` for environment variables or `env.get_wrapper_attr('config')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# prints env configs\n",
    "#* obs is flattened to 1D array for nn\n",
    "\n",
    "pprint.pprint(env.config)\n",
    "(obs, info), done = env.reset(), False\n",
    "obs = np.array(obs.flatten())\n",
    "print(\"Environment is setted up.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               5888      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 9)                 2313      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 73,993\n",
      "Trainable params: 73,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Agent is initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 22:00:48.703563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-10 22:00:48.703691: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-10-10 22:00:48.703729: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-10-10 22:00:48.703752: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-10-10 22:00:48.703772: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-10-10 22:00:48.703792: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-10-10 22:00:48.703812: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-10-10 22:00:48.703832: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-10-10 22:00:48.703851: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-10-10 22:00:48.703854: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-10-10 22:00:48.704619: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# agent config\n",
    "\n",
    "agent = DDQNAgent(alpha=0.001, gamma=0.9, epsilon=1.0, obs_shape=obs.shape,\n",
    "                  batch_size=64, epsilon_dec=0.999, epsilon_end=0.05, mem_size=20000,\n",
    "                  min_mem_size=100, replace_target=100, learning_rate=0.001)\n",
    "\n",
    "print(\"Agent is initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* basic params for training\n",
    "\n",
    "best_score = -1000.0\n",
    "score_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/o/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/o/Documents/thesis/highway/videos/trainings/ddqn folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e4ca079d974e17a0d5ab7166b41461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test episodes:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'speed': 10.0, 'crashed': False, 'action': [0.5], 'rewards': {'lane_centering_reward': 0.48973024579148555, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.5], 'rewards': {'lane_centering_reward': 0.7110401466141923, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.5], 'rewards': {'lane_centering_reward': 0.2666831819141909, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.0], 'rewards': {'lane_centering_reward': 0.1584757195319505, 'action_reward': 0.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.75], 'rewards': {'lane_centering_reward': 0.0479688389186827, 'action_reward': 0.75, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.25], 'rewards': {'lane_centering_reward': 0.0491024471772, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-1.0], 'rewards': {'lane_centering_reward': 0.04459580128207081, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.0], 'rewards': {'lane_centering_reward': 0.046198509461712584, 'action_reward': 0.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.75], 'rewards': {'lane_centering_reward': 0.1071258343356352, 'action_reward': 0.75, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.0], 'rewards': {'lane_centering_reward': 0.23545607926868367, 'action_reward': 0.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.75], 'rewards': {'lane_centering_reward': 0.663413688363338, 'action_reward': 0.75, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.5], 'rewards': {'lane_centering_reward': 0.07386842788387393, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-1.0], 'rewards': {'lane_centering_reward': 0.01831309153193929, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': False}}\n",
      "-----saving models------\n",
      "episode  0 score 5.6 avg score 5.6\n",
      "Exp- value: 1.0\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-1.0], 'rewards': {'lane_centering_reward': 0.17235015561053746, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [1.0], 'rewards': {'lane_centering_reward': 0.3707927819740454, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.75], 'rewards': {'lane_centering_reward': 0.10745422935519769, 'action_reward': 0.75, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.75], 'rewards': {'lane_centering_reward': 0.03400271828178351, 'action_reward': 0.75, 'collision_reward': False, 'on_road_reward': False}}\n",
      "episode  1 score 0.4 avg score 3.0\n",
      "Exp- value: 1.0\n",
      "{'speed': 10.0, 'crashed': False, 'action': [1.0], 'rewards': {'lane_centering_reward': 0.17235015561053746, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.5], 'rewards': {'lane_centering_reward': 0.0470496813754191, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.25], 'rewards': {'lane_centering_reward': 0.05987399258948158, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.25], 'rewards': {'lane_centering_reward': 0.23252029081249936, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-1.0], 'rewards': {'lane_centering_reward': 0.1896649663717501, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.0], 'rewards': {'lane_centering_reward': 0.3312510071345991, 'action_reward': 0.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [1.0], 'rewards': {'lane_centering_reward': 0.3795289159997245, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.0], 'rewards': {'lane_centering_reward': 0.08570511776228613, 'action_reward': 0.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.75], 'rewards': {'lane_centering_reward': 0.02341800095192784, 'action_reward': 0.75, 'collision_reward': False, 'on_road_reward': False}}\n",
      "episode  2 score 3.1 avg score 3.1\n",
      "Exp- value: 1.0\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-1.0], 'rewards': {'lane_centering_reward': 0.17235015561053751, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-1.0], 'rewards': {'lane_centering_reward': 0.0452747356505048, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.5], 'rewards': {'lane_centering_reward': 0.3764109839373442, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.5], 'rewards': {'lane_centering_reward': 0.483733918363061, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.5], 'rewards': {'lane_centering_reward': 0.05045721315385002, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.5], 'rewards': {'lane_centering_reward': 0.022059284336212324, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': False}}\n",
      "episode  3 score 1.5 avg score 2.7\n",
      "Exp- value: 1.0\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.25], 'rewards': {'lane_centering_reward': 0.799582107257907, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.75], 'rewards': {'lane_centering_reward': 0.6193503543875583, 'action_reward': 0.75, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [1.0], 'rewards': {'lane_centering_reward': 0.074735727549731, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.75], 'rewards': {'lane_centering_reward': 0.05836237867460137, 'action_reward': 0.75, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.5], 'rewards': {'lane_centering_reward': 0.060538247776734215, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.25], 'rewards': {'lane_centering_reward': 0.12069282713245291, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.25], 'rewards': {'lane_centering_reward': 0.5938287648162929, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-1.0], 'rewards': {'lane_centering_reward': 0.3408241993157683, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.75], 'rewards': {'lane_centering_reward': 0.7488986382815352, 'action_reward': 0.75, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.0], 'rewards': {'lane_centering_reward': 0.16429863776505602, 'action_reward': 0.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.25], 'rewards': {'lane_centering_reward': 0.04529588916567319, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.5], 'rewards': {'lane_centering_reward': 0.026307227220494032, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': False}}\n",
      "episode  4 score 5.5 avg score 3.2\n",
      "Exp- value: 1.0\n",
      "{'speed': 10.0, 'crashed': False, 'action': [1.0], 'rewards': {'lane_centering_reward': 0.17235015561053746, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.25], 'rewards': {'lane_centering_reward': 0.09360224353424972, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.25], 'rewards': {'lane_centering_reward': 0.04358690639134028, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.25], 'rewards': {'lane_centering_reward': 0.07720088825529535, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-1.0], 'rewards': {'lane_centering_reward': 0.057829680004043316, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.25], 'rewards': {'lane_centering_reward': 0.08779069017174525, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.75], 'rewards': {'lane_centering_reward': 0.45886264594999554, 'action_reward': 0.75, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.5], 'rewards': {'lane_centering_reward': 0.3391100691739549, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.5], 'rewards': {'lane_centering_reward': 0.12571954283823714, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.25], 'rewards': {'lane_centering_reward': 0.04359014919002713, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.5], 'rewards': {'lane_centering_reward': 0.030841225217811186, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': False}}\n",
      "episode  5 score 4.0 avg score 3.4\n",
      "Exp- value: 1.0\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-1.0], 'rewards': {'lane_centering_reward': 0.17235015561053746, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.5], 'rewards': {'lane_centering_reward': 0.1298891548518175, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.5], 'rewards': {'lane_centering_reward': 0.15470076534993327, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.75], 'rewards': {'lane_centering_reward': 0.05778458432376564, 'action_reward': 0.75, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.25], 'rewards': {'lane_centering_reward': 0.04366111934620823, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.75], 'rewards': {'lane_centering_reward': 0.020665784005205617, 'action_reward': 0.75, 'collision_reward': False, 'on_road_reward': False}}\n",
      "episode  6 score 1.3 avg score 3.1\n",
      "Exp- value: 1.0\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.25], 'rewards': {'lane_centering_reward': 0.799582107257907, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.75], 'rewards': {'lane_centering_reward': 0.6193503543875583, 'action_reward': 0.75, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.25], 'rewards': {'lane_centering_reward': 0.5105856632407905, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.25], 'rewards': {'lane_centering_reward': 0.22528916522254014, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.5], 'rewards': {'lane_centering_reward': 0.30230019666735103, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.25], 'rewards': {'lane_centering_reward': 0.46784472159878004, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.5], 'rewards': {'lane_centering_reward': 0.23072752730477816, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.25], 'rewards': {'lane_centering_reward': 0.25605744316779866, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-1.0], 'rewards': {'lane_centering_reward': 0.8418126474721535, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.75], 'rewards': {'lane_centering_reward': 0.9706945106347155, 'action_reward': 0.75, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-1.0], 'rewards': {'lane_centering_reward': 0.12467550740235006, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [1.0], 'rewards': {'lane_centering_reward': 0.1838952993497625, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.75], 'rewards': {'lane_centering_reward': 0.05873968860471689, 'action_reward': 0.75, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-1.0], 'rewards': {'lane_centering_reward': 0.019019947117001945, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': False}}\n",
      "episode  7 score 7.2 avg score 3.6\n",
      "Exp- value: 1.0\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.0], 'rewards': {'lane_centering_reward': 1.0, 'action_reward': 0.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.5], 'rewards': {'lane_centering_reward': 0.4897302457914847, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-1.0], 'rewards': {'lane_centering_reward': 0.06738490416305307, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-1.0], 'rewards': {'lane_centering_reward': 0.11856546119914266, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [1.0], 'rewards': {'lane_centering_reward': 0.2887207180491602, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.25], 'rewards': {'lane_centering_reward': 0.5884014769454559, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.5], 'rewards': {'lane_centering_reward': 0.0629487562348706, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.0], 'rewards': {'lane_centering_reward': 0.022322074361522014, 'action_reward': 0.0, 'collision_reward': False, 'on_road_reward': False}}\n",
      "episode  8 score 3.2 avg score 3.5\n",
      "Exp- value: 1.0\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.75], 'rewards': {'lane_centering_reward': 0.2861629067323223, 'action_reward': 0.75, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-1.0], 'rewards': {'lane_centering_reward': 0.046382569672988004, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [1.0], 'rewards': {'lane_centering_reward': 0.04425469274368287, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.75], 'rewards': {'lane_centering_reward': 0.10595525319535191, 'action_reward': 0.75, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.5], 'rewards': {'lane_centering_reward': 0.21317180317721243, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.25], 'rewards': {'lane_centering_reward': 0.47491712488206755, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.75], 'rewards': {'lane_centering_reward': 0.2774750356102331, 'action_reward': 0.75, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.5], 'rewards': {'lane_centering_reward': 0.7701450532655918, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.75], 'rewards': {'lane_centering_reward': 0.3174934195271166, 'action_reward': 0.75, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.75], 'rewards': {'lane_centering_reward': 0.9680872872510824, 'action_reward': 0.75, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.5], 'rewards': {'lane_centering_reward': 0.7892497683861514, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.5], 'rewards': {'lane_centering_reward': 0.26351122070949984, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.75], 'rewards': {'lane_centering_reward': 0.05359707324572732, 'action_reward': 0.75, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.25], 'rewards': {'lane_centering_reward': 0.09218657680382257, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.0], 'rewards': {'lane_centering_reward': 0.8455084206670763, 'action_reward': 0.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [1.0], 'rewards': {'lane_centering_reward': 0.07667866719399663, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.5], 'rewards': {'lane_centering_reward': 0.022237285137636162, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': False}}\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.5959 - accuracy: 0.6562\n",
      "episode  9 score 8.3 avg score 4.0\n",
      "Exp- value: 0.999\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.5], 'rewards': {'lane_centering_reward': 0.48973024579148555, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2983 - accuracy: 0.6875\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.25], 'rewards': {'lane_centering_reward': 0.4231511666384459, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.5148 - accuracy: 0.4844\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-1.0], 'rewards': {'lane_centering_reward': 0.64586554599088, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.9625 - accuracy: 0.5312\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.75], 'rewards': {'lane_centering_reward': 0.9560668930533927, 'action_reward': 0.75, 'collision_reward': False, 'on_road_reward': True}}\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.8172 - accuracy: 0.4844\n",
      "{'speed': 10.0, 'crashed': False, 'action': [1.0], 'rewards': {'lane_centering_reward': 0.21584136358129927, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5035 - accuracy: 0.6562\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.0], 'rewards': {'lane_centering_reward': 0.0876131313575624, 'action_reward': 0.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4387 - accuracy: 0.4219\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.25], 'rewards': {'lane_centering_reward': 0.03860844675742246, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2646 - accuracy: 0.7500\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.25], 'rewards': {'lane_centering_reward': 0.10998905342149252, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1584 - accuracy: 0.6094\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.25], 'rewards': {'lane_centering_reward': 0.9492383589911384, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1746 - accuracy: 0.4062\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.0], 'rewards': {'lane_centering_reward': 0.14530923236330642, 'action_reward': 0.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1691 - accuracy: 0.3750\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.5], 'rewards': {'lane_centering_reward': 0.048200405407728734, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1399 - accuracy: 0.3125\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.0], 'rewards': {'lane_centering_reward': 0.020132474748186927, 'action_reward': 0.0, 'collision_reward': False, 'on_road_reward': False}}\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6287 - accuracy: 0.4219\n",
      "episode  10 score 5.8 avg score 4.2\n",
      "Exp- value: 0.9870777147137147\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.75], 'rewards': {'lane_centering_reward': 0.28616290673232214, 'action_reward': 0.75, 'collision_reward': False, 'on_road_reward': True}}\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.8326 - accuracy: 0.5781\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.25], 'rewards': {'lane_centering_reward': 0.09654286213739967, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0402 - accuracy: 0.7500\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.0], 'rewards': {'lane_centering_reward': 0.049859004694877, 'action_reward': 0.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.8628 - accuracy: 0.5469\n",
      "{'speed': 10.0, 'crashed': False, 'action': [1.0], 'rewards': {'lane_centering_reward': 0.08050615808296833, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.9209 - accuracy: 0.8281\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.5], 'rewards': {'lane_centering_reward': 0.051239746581122465, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7516 - accuracy: 0.7188\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-1.0], 'rewards': {'lane_centering_reward': 0.020408011197083704, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': False}}\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.5043 - accuracy: 0.4688\n",
      "episode  11 score 1.4 avg score 4.0\n",
      "Exp- value: 0.9811700348643991\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.0], 'rewards': {'lane_centering_reward': 1.0, 'action_reward': 0.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2322 - accuracy: 0.6875\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.75], 'rewards': {'lane_centering_reward': 0.28616290673232214, 'action_reward': 0.75, 'collision_reward': False, 'on_road_reward': True}}\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2862 - accuracy: 0.6562\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.25], 'rewards': {'lane_centering_reward': 0.18631537014588012, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2674 - accuracy: 0.7500\n",
      "{'speed': 10.0, 'crashed': False, 'action': [1.0], 'rewards': {'lane_centering_reward': 0.0411945175363398, 'action_reward': 1.0, 'collision_reward': False, 'on_road_reward': True}}\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.3463 - accuracy: 0.5781\n",
      "{'speed': 10.0, 'crashed': False, 'action': [-0.5], 'rewards': {'lane_centering_reward': 0.05627555885297077, 'action_reward': 0.5, 'collision_reward': False, 'on_road_reward': True}}\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6051 - accuracy: 0.5156\n",
      "{'speed': 10.0, 'crashed': False, 'action': [0.25], 'rewards': {'lane_centering_reward': 0.1719989128215252, 'action_reward': 0.25, 'collision_reward': False, 'on_road_reward': True}}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:5306\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   5303\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   5304\u001b[0m   \u001b[39m# TODO(apassos) find a less bad way of detecting resource variables\u001b[39;00m\n\u001b[1;32m   5305\u001b[0m   \u001b[39m# without introducing a circular dependency.\u001b[39;00m\n\u001b[0;32m-> 5306\u001b[0m   \u001b[39mreturn\u001b[39;00m params\u001b[39m.\u001b[39;49msparse_read(indices, name\u001b[39m=\u001b[39mname)\n\u001b[1;32m   5307\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:444\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    438\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    439\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m    440\u001b[0m \u001b[39m    If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[1;32m    441\u001b[0m \u001b[39m    from tensorflow.python.ops.numpy_ops import np_config\u001b[39m\n\u001b[1;32m    442\u001b[0m \u001b[39m    np_config.enable_numpy_behavior()\u001b[39m\n\u001b[1;32m    443\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"\u001b[39m)\n\u001b[0;32m--> 444\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'sparse_read'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/o/Documents/thesis/highway/racetrack.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/o/Documents/thesis/highway/racetrack.ipynb#X12sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/o/Documents/thesis/highway/racetrack.ipynb#X12sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     agent\u001b[39m.\u001b[39mremember(state\u001b[39m=\u001b[39mobservation, action\u001b[39m=\u001b[39maction_index, done\u001b[39m=\u001b[39mdone_,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/o/Documents/thesis/highway/racetrack.ipynb#X12sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m                     reward\u001b[39m=\u001b[39mreward, new_state\u001b[39m=\u001b[39mnew_observation)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/o/Documents/thesis/highway/racetrack.ipynb#X12sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     agent\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/o/Documents/thesis/highway/racetrack.ipynb#X12sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     observation \u001b[39m=\u001b[39m new_observation\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/o/Documents/thesis/highway/racetrack.ipynb#X12sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m score_history\u001b[39m.\u001b[39mappend(score)\n",
      "\u001b[1;32m/home/o/Documents/thesis/highway/racetrack.ipynb Cell 10\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/o/Documents/thesis/highway/racetrack.ipynb#X12sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m action_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(action, action_values)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/o/Documents/thesis/highway/racetrack.ipynb#X12sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39m# state = tf.convert_to_tensor(state, dtype = tf.float16)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/o/Documents/thesis/highway/racetrack.ipynb#X12sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39m# new_state = tf.convert_to_tensor(new_state, dtype = tf.float16)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/o/Documents/thesis/highway/racetrack.ipynb#X12sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39m# reward = tf.convert_to_tensor(reward, dtype = tf.float16)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/o/Documents/thesis/highway/racetrack.ipynb#X12sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/o/Documents/thesis/highway/racetrack.ipynb#X12sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39m#* get the q values of current states by main network\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/o/Documents/thesis/highway/racetrack.ipynb#X12sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m q_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_eval\u001b[39m.\u001b[39;49mpredict(state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/o/Documents/thesis/highway/racetrack.ipynb#X12sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39m#! for abs error\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/o/Documents/thesis/highway/racetrack.ipynb#X12sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m target_old \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(q_pred)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/keras/engine/training.py:2317\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2308\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m   2309\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   2310\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2311\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2314\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m   2315\u001b[0m         )\n\u001b[0;32m-> 2317\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[1;32m   2318\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m   2319\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   2320\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[1;32m   2321\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m   2322\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m   2323\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   2324\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   2325\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   2326\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2327\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution,\n\u001b[1;32m   2328\u001b[0m )\n\u001b[1;32m   2330\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   2331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/keras/engine/data_adapter.py:1579\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1577\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1578\u001b[0m     \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 1579\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/keras/engine/data_adapter.py:1259\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1256\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[1;32m   1258\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1259\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[1;32m   1260\u001b[0m     x,\n\u001b[1;32m   1261\u001b[0m     y,\n\u001b[1;32m   1262\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   1263\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m   1264\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[1;32m   1265\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1266\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m   1267\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   1268\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   1269\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   1270\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[1;32m   1271\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1272\u001b[0m )\n\u001b[1;32m   1274\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[1;32m   1276\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/keras/engine/data_adapter.py:347\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[39mreturn\u001b[39;00m flat_dataset\n\u001b[1;32m    345\u001b[0m indices_dataset \u001b[39m=\u001b[39m indices_dataset\u001b[39m.\u001b[39mflat_map(slice_batch_indices)\n\u001b[0;32m--> 347\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mslice_inputs(indices_dataset, inputs)\n\u001b[1;32m    349\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    351\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mshuffle_batch\u001b[39m(\u001b[39m*\u001b[39mbatch):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/keras/engine/data_adapter.py:388\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrab_batch\u001b[39m(i, data):\n\u001b[1;32m    384\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(\n\u001b[1;32m    385\u001b[0m         \u001b[39mlambda\u001b[39;00m d: tf\u001b[39m.\u001b[39mgather(d, i, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), data\n\u001b[1;32m    386\u001b[0m     )\n\u001b[0;32m--> 388\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(grab_batch, num_parallel_calls\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mAUTOTUNE)\n\u001b[1;32m    390\u001b[0m \u001b[39m# Default optimizations are disabled to avoid the overhead of\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[39m# (unnecessary) input pipeline graph serialization and deserialization\u001b[39;00m\n\u001b[1;32m    392\u001b[0m options \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mOptions()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:2296\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2294\u001b[0m   \u001b[39mreturn\u001b[39;00m MapDataset(\u001b[39mself\u001b[39m, map_func, preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, name\u001b[39m=\u001b[39mname)\n\u001b[1;32m   2295\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2296\u001b[0m   \u001b[39mreturn\u001b[39;00m ParallelMapDataset(\n\u001b[1;32m   2297\u001b[0m       \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2298\u001b[0m       map_func,\n\u001b[1;32m   2299\u001b[0m       num_parallel_calls,\n\u001b[1;32m   2300\u001b[0m       deterministic,\n\u001b[1;32m   2301\u001b[0m       preserve_cardinality\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2302\u001b[0m       name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:5540\u001b[0m, in \u001b[0;36mParallelMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m   5538\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_dataset \u001b[39m=\u001b[39m input_dataset\n\u001b[1;32m   5539\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_inter_op_parallelism \u001b[39m=\u001b[39m use_inter_op_parallelism\n\u001b[0;32m-> 5540\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m structured_function\u001b[39m.\u001b[39;49mStructuredFunctionWrapper(\n\u001b[1;32m   5541\u001b[0m     map_func,\n\u001b[1;32m   5542\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transformation_name(),\n\u001b[1;32m   5543\u001b[0m     dataset\u001b[39m=\u001b[39;49minput_dataset,\n\u001b[1;32m   5544\u001b[0m     use_legacy_function\u001b[39m=\u001b[39;49muse_legacy_function)\n\u001b[1;32m   5545\u001b[0m \u001b[39mif\u001b[39;00m deterministic \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5546\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deterministic \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:263\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m       warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    257\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    259\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    261\u001b[0m     fn_factory \u001b[39m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 263\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function \u001b[39m=\u001b[39m fn_factory()\n\u001b[1;32m    264\u001b[0m \u001b[39m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m add_to_graph \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:226\u001b[0m, in \u001b[0;36mTracingCompiler.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    218\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \n\u001b[1;32m    220\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39m      `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m   concrete_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_concrete_function_garbage_collected(\n\u001b[1;32m    227\u001b[0m       \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    228\u001b[0m   concrete_function\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    229\u001b[0m   \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:192\u001b[0m, in \u001b[0;36mTracingCompiler._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_spec\u001b[39m.\u001b[39mvalidate_inputs_with_signature(args, kwargs)\n\u001b[1;32m    191\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m--> 192\u001b[0m   concrete_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_concrete_function(args, kwargs)\n\u001b[1;32m    193\u001b[0m   seen_names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m    194\u001b[0m   captured \u001b[39m=\u001b[39m object_identity\u001b[39m.\u001b[39mObjectIdentitySet(\n\u001b[1;32m    195\u001b[0m       concrete_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39minternal_captures)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:157\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m   args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature\n\u001b[1;32m    155\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 157\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:360\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m   \u001b[39m# Only get placeholders for arguments, not captures\u001b[39;00m\n\u001b[1;32m    358\u001b[0m   args, kwargs \u001b[39m=\u001b[39m generalized_func_key\u001b[39m.\u001b[39m_placeholder_value()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m concrete_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_concrete_function(args, kwargs)\n\u001b[1;32m    362\u001b[0m graph_capture_container \u001b[39m=\u001b[39m concrete_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39m_capture_func_lib  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39m# Maintain the list of all captures\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:284\u001b[0m, in \u001b[0;36mTracingCompiler._create_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[1;32m    280\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m    281\u001b[0m ]\n\u001b[1;32m    282\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[1;32m    283\u001b[0m concrete_function \u001b[39m=\u001b[39m monomorphic_function\u001b[39m.\u001b[39mConcreteFunction(\n\u001b[0;32m--> 284\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[1;32m    285\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m    286\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[1;32m    287\u001b[0m         args,\n\u001b[1;32m    288\u001b[0m         kwargs,\n\u001b[1;32m    289\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    290\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[1;32m    291\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[1;32m    292\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[1;32m    293\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[1;32m    294\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[1;32m    295\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[1;32m    296\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m    297\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m    298\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    301\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1283\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1281\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1283\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[1;32m   1285\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m func_outputs \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:240\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39m@eager_function\u001b[39m\u001b[39m.\u001b[39mdefun_with_attributes(\n\u001b[1;32m    235\u001b[0m     input_signature\u001b[39m=\u001b[39mstructure\u001b[39m.\u001b[39mget_flat_tensor_specs(\n\u001b[1;32m    236\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_structure),\n\u001b[1;32m    237\u001b[0m     autograph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    238\u001b[0m     attributes\u001b[39m=\u001b[39mdefun_kwargs)\n\u001b[1;32m    239\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_fn\u001b[39m(\u001b[39m*\u001b[39margs):  \u001b[39m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m   ret \u001b[39m=\u001b[39m wrapper_helper(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    241\u001b[0m   ret \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_tensor_list(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_structure, ret)\n\u001b[1;32m    242\u001b[0m   \u001b[39mreturn\u001b[39;00m [ops\u001b[39m.\u001b[39mconvert_to_tensor(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m ret]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:171\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[1;32m    170\u001b[0m   nested_args \u001b[39m=\u001b[39m (nested_args,)\n\u001b[0;32m--> 171\u001b[0m ret \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39;49mtf_convert(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func, ag_ctx)(\u001b[39m*\u001b[39;49mnested_args)\n\u001b[1;32m    172\u001b[0m ret \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(ret)\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m _should_pack(ret):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 689\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_allowlisted(f):\n\u001b[0;32m--> 377\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    379\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:458\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    455\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    457\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 458\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    459\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/keras/engine/data_adapter.py:384\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs.<locals>.grab_batch\u001b[0;34m(i, data)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrab_batch\u001b[39m(i, data):\n\u001b[0;32m--> 384\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(\n\u001b[1;32m    385\u001b[0m         \u001b[39mlambda\u001b[39;49;00m d: tf\u001b[39m.\u001b[39;49mgather(d, i, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m), data\n\u001b[1;32m    386\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/keras/engine/data_adapter.py:385\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs.<locals>.grab_batch.<locals>.<lambda>\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrab_batch\u001b[39m(i, data):\n\u001b[1;32m    384\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(\n\u001b[0;32m--> 385\u001b[0m         \u001b[39mlambda\u001b[39;00m d: tf\u001b[39m.\u001b[39;49mgather(d, i, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m), data\n\u001b[1;32m    386\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:5319\u001b[0m, in \u001b[0;36mgather_v2\u001b[0;34m(params, indices, validate_indices, axis, batch_dims, name)\u001b[0m\n\u001b[1;32m   5311\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mgather\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m   5312\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[1;32m   5313\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgather_v2\u001b[39m(params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5317\u001b[0m               batch_dims\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m   5318\u001b[0m               name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 5319\u001b[0m   \u001b[39mreturn\u001b[39;00m gather(\n\u001b[1;32m   5320\u001b[0m       params,\n\u001b[1;32m   5321\u001b[0m       indices,\n\u001b[1;32m   5322\u001b[0m       validate_indices\u001b[39m=\u001b[39;49mvalidate_indices,\n\u001b[1;32m   5323\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   5324\u001b[0m       axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   5325\u001b[0m       batch_dims\u001b[39m=\u001b[39;49mbatch_dims)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py:561\u001b[0m, in \u001b[0;36mdeprecated_args.<locals>.deprecated_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m         _PRINTED_WARNING[(func, arg_name)] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    554\u001b[0m       logging\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    555\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mFrom \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: calling \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m (from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) with \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is deprecated and will \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    556\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mbe removed \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mInstructions for updating:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    559\u001b[0m           \u001b[39m'\u001b[39m\u001b[39min a future version\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m date \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mafter \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m date),\n\u001b[1;32m    560\u001b[0m           instructions)\n\u001b[0;32m--> 561\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:5308\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   5306\u001b[0m   \u001b[39mreturn\u001b[39;00m params\u001b[39m.\u001b[39msparse_read(indices, name\u001b[39m=\u001b[39mname)\n\u001b[1;32m   5307\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m-> 5308\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_array_ops\u001b[39m.\u001b[39;49mgather_v2(params, indices, axis, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/ops/gen_array_ops.py:3957\u001b[0m, in \u001b[0;36mgather_v2\u001b[0;34m(params, indices, axis, batch_dims, name)\u001b[0m\n\u001b[1;32m   3955\u001b[0m   batch_dims \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m   3956\u001b[0m batch_dims \u001b[39m=\u001b[39m _execute\u001b[39m.\u001b[39mmake_int(batch_dims, \u001b[39m\"\u001b[39m\u001b[39mbatch_dims\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 3957\u001b[0m _, _, _op, _outputs \u001b[39m=\u001b[39m _op_def_library\u001b[39m.\u001b[39;49m_apply_op_helper(\n\u001b[1;32m   3958\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mGatherV2\u001b[39;49m\u001b[39m\"\u001b[39;49m, params\u001b[39m=\u001b[39;49mparams, indices\u001b[39m=\u001b[39;49mindices, axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   3959\u001b[0m                   batch_dims\u001b[39m=\u001b[39;49mbatch_dims, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   3960\u001b[0m _result \u001b[39m=\u001b[39m _outputs[:]\n\u001b[1;32m   3961\u001b[0m \u001b[39mif\u001b[39;00m _execute\u001b[39m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/framework/op_def_library.py:777\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[39mwith\u001b[39;00m g\u001b[39m.\u001b[39mas_default(), ops\u001b[39m.\u001b[39mname_scope(name) \u001b[39mas\u001b[39;00m scope:\n\u001b[1;32m    776\u001b[0m   \u001b[39mif\u001b[39;00m fallback:\n\u001b[0;32m--> 777\u001b[0m     _ExtractInputsAndAttrs(op_type_name, op_def, allowed_list_attr_map,\n\u001b[1;32m    778\u001b[0m                            keywords, default_type_attr_map, attrs, inputs,\n\u001b[1;32m    779\u001b[0m                            input_types)\n\u001b[1;32m    780\u001b[0m     _ExtractRemainingAttrs(op_type_name, op_def, keywords,\n\u001b[1;32m    781\u001b[0m                            default_type_attr_map, attrs)\n\u001b[1;32m    782\u001b[0m     _ExtractAttrProto(op_type_name, op_def, attrs, attr_protos)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/framework/op_def_library.py:550\u001b[0m, in \u001b[0;36m_ExtractInputsAndAttrs\u001b[0;34m(op_type_name, op_def, allowed_list_attr_map, keywords, default_type_attr_map, attrs, inputs, input_types)\u001b[0m\n\u001b[1;32m    544\u001b[0m       values \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mconvert_to_tensor(\n\u001b[1;32m    545\u001b[0m           values,\n\u001b[1;32m    546\u001b[0m           name\u001b[39m=\u001b[39minput_arg\u001b[39m.\u001b[39mname,\n\u001b[1;32m    547\u001b[0m           as_ref\u001b[39m=\u001b[39minput_arg\u001b[39m.\u001b[39mis_ref,\n\u001b[1;32m    548\u001b[0m           preferred_dtype\u001b[39m=\u001b[39mdefault_dtype)\n\u001b[1;32m    549\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m     values \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(\n\u001b[1;32m    551\u001b[0m         values,\n\u001b[1;32m    552\u001b[0m         name\u001b[39m=\u001b[39;49minput_arg\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    553\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    554\u001b[0m         as_ref\u001b[39m=\u001b[39;49minput_arg\u001b[39m.\u001b[39;49mis_ref,\n\u001b[1;32m    555\u001b[0m         preferred_dtype\u001b[39m=\u001b[39;49mdefault_dtype)\n\u001b[1;32m    556\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    557\u001b[0m   \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1636\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1627\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1628\u001b[0m           _add_error_prefix(\n\u001b[1;32m   1629\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1632\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1633\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[1;32m   1635\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1636\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m   1638\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1639\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:48\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[1;32m     47\u001b[0m   \u001b[39mdel\u001b[39;00m as_ref  \u001b[39m# Unused.\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m   \u001b[39mreturn\u001b[39;00m constant_op\u001b[39m.\u001b[39;49mconstant(value, dtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    172\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \n\u001b[1;32m    174\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    268\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:284\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    281\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[1;32m    282\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n\u001b[1;32m    283\u001b[0m tensor_value\u001b[39m.\u001b[39mtensor\u001b[39m.\u001b[39mCopyFrom(\n\u001b[0;32m--> 284\u001b[0m     tensor_util\u001b[39m.\u001b[39;49mmake_tensor_proto(\n\u001b[1;32m    285\u001b[0m         value, dtype\u001b[39m=\u001b[39;49mdtype, shape\u001b[39m=\u001b[39;49mshape, verify_shape\u001b[39m=\u001b[39;49mverify_shape,\n\u001b[1;32m    286\u001b[0m         allow_broadcast\u001b[39m=\u001b[39;49mallow_broadcast))\n\u001b[1;32m    287\u001b[0m dtype_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue(\u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mtensor_value\u001b[39m.\u001b[39mtensor\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    288\u001b[0m attrs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m: tensor_value, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m: dtype_value}\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/framework/tensor_util.py:470\u001b[0m, in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    468\u001b[0m     downcasted_array \u001b[39m=\u001b[39m nparray\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mint32)\n\u001b[1;32m    469\u001b[0m     \u001b[39m# Do not down cast if it leads to precision loss.\u001b[39;00m\n\u001b[0;32m--> 470\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39;49marray_equal(downcasted_array, nparray):\n\u001b[1;32m    471\u001b[0m       nparray \u001b[39m=\u001b[39m downcasted_array\n\u001b[1;32m    473\u001b[0m \u001b[39m# if dtype is provided, it must be compatible with what numpy\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39m# conversion says.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/numpy/core/numeric.py:2439\u001b[0m, in \u001b[0;36marray_equal\u001b[0;34m(a1, a2, equal_nan)\u001b[0m\n\u001b[1;32m   2437\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   2438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m equal_nan:\n\u001b[0;32m-> 2439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(asarray(a1 \u001b[39m==\u001b[39;49m a2)\u001b[39m.\u001b[39;49mall())\n\u001b[1;32m   2440\u001b[0m \u001b[39m# Handling NaN values if equal_nan is True\u001b[39;00m\n\u001b[1;32m   2441\u001b[0m a1nan, a2nan \u001b[39m=\u001b[39m isnan(a1), isnan(a2)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/numpy/core/_methods.py:61\u001b[0m, in \u001b[0;36m_all\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[39mreturn\u001b[39;00m umr_any(a, axis, dtype, out, keepdims)\n\u001b[1;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m umr_any(a, axis, dtype, out, keepdims, where\u001b[39m=\u001b[39mwhere)\n\u001b[0;32m---> 61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_all\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     62\u001b[0m     \u001b[39m# Parsing keyword arguments is currently fairly slow, so avoid it for now\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[39mif\u001b[39;00m where \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m         \u001b[39mreturn\u001b[39;00m umr_all(a, axis, dtype, out, keepdims)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main loop\n",
    "\n",
    "#TODO-1: Parameters Tuning\n",
    "\n",
    "#TODO-6: Actions Shaping\n",
    "\n",
    "#TODO-7: Pruning\n",
    "\n",
    "#TODO-8: Tensorboard\n",
    "\n",
    "env = record_videos(env)\n",
    "\n",
    "for episode in trange(2000, desc='Test episodes'):\n",
    "        (observation, info), done = env.reset(), False\n",
    "        observation = np.array(observation.flatten())\n",
    "\n",
    "\n",
    "        done_ = False\n",
    "        score = 0\n",
    "        step = 0\n",
    "        truncated = False\n",
    "        # env.render()\n",
    "        while not done_:\n",
    "            action, action_index = agent.get_action(observation.reshape((1,observation.shape[0])))\n",
    "            new_observation, reward, done, truncated, info = env.step(action=[action])\n",
    "            new_observation = np.array(new_observation.flatten())\n",
    "\n",
    "            if info[\"crashed\"] == True or info[\"rewards\"][\"on_road_reward\"] == False or truncated == True:\n",
    "                done_ = True\n",
    "                reward = -1.0\n",
    "            else: done_ = False\n",
    "\n",
    "            score += reward\n",
    "\n",
    "            agent.remember(state=observation, action=action_index, done=done_,\n",
    "                            reward=reward, new_state=new_observation)\n",
    "            agent.train()\n",
    "\n",
    "            observation = new_observation\n",
    "            \n",
    "\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            agent.save_model(episode)\n",
    "\n",
    "        print('episode ', episode, 'score %.1f' % score,\n",
    "               'avg score %.1f' % avg_score)\n",
    "        print(\"Exp- value:\", agent.epsilon)\n",
    "        time.sleep(1)\n",
    "\n",
    "env.close()\n",
    "# show_videos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.save(\"score_history\", np.array(score_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.02133133098047\n",
      "1.974266679505516\n"
     ]
    }
   ],
   "source": [
    "print(best_score)\n",
    "print(avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----loading models------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/o/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/o/Documents/thesis/highway/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05260480bc24a238c5aabf9662e532a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test episodes:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0\n",
      "Score:  1.3721311968406065\n",
      "Episode:  1\n",
      "Score:  5.0530271586894075\n",
      "Episode:  2\n",
      "Score:  6.577469628791425\n",
      "Episode:  3\n",
      "Score:  -0.05288229273136713\n",
      "Episode:  4\n",
      "Score:  0.775702045658262\n",
      "Episode:  5\n",
      "Score:  4.964922426753592\n",
      "Episode:  6\n",
      "Score:  9.828439108016884\n",
      "Episode:  7\n",
      "Score:  5.319425406038121\n",
      "Episode:  8\n",
      "Score:  2.6945582932489436\n",
      "Episode:  9\n",
      "Score:  1.8040322219631442\n",
      "Moviepy - Building video /home/o/Documents/thesis/highway/videos/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/o/Documents/thesis/highway/videos/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/o/Documents/thesis/highway/videos/rl-video-episode-0.mp4\n",
      "Moviepy - Building video /home/o/Documents/thesis/highway/videos/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/o/Documents/thesis/highway/videos/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/o/Documents/thesis/highway/videos/rl-video-episode-0.mp4\n",
      "Moviepy - Building video /home/o/Documents/thesis/highway/videos/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/o/Documents/thesis/highway/videos/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/o/Documents/thesis/highway/videos/rl-video-episode-0.mp4\n",
      "Moviepy - Building video /home/o/Documents/thesis/highway/videos/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/o/Documents/thesis/highway/videos/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/o/Documents/thesis/highway/videos/rl-video-episode-0.mp4\n"
     ]
    }
   ],
   "source": [
    "#Evaluating\n",
    "agent.load_model()\n",
    "\n",
    "env = record_videos(env)\n",
    "\n",
    "for episode in trange(10, desc='Test episodes'):\n",
    "        (observation, info), done = env.reset(), False\n",
    "        observation = np.array(observation.flatten())\n",
    "\n",
    "        done_ = False\n",
    "        score = 0\n",
    "        step = 0\n",
    "        # env.render()\n",
    "        while True:\n",
    "            action, action_index = agent.get_action(observation.reshape((1,observation.shape[0])))\n",
    "            new_observation, reward, done, truncated, info = env.step(action=[action])\n",
    "            new_observation = np.array(new_observation.flatten())\n",
    "\n",
    "            if info[\"crashed\"] == True or info[\"rewards\"][\"on_road_reward\"] == False:\n",
    "                done_ = True\n",
    "                reward = -1.0\n",
    "            else: done_ = False\n",
    "\n",
    "            score += reward\n",
    "\n",
    "\n",
    "            observation = new_observation\n",
    "\n",
    "            if done or done_:\n",
    "                break\n",
    "\n",
    "\n",
    "        print(\"Episode: \", episode)\n",
    "        print(\"Score: \", score)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
