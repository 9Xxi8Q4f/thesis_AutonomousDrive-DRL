{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 01:29:09.904261: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-12 01:29:09.927463: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-12 01:29:09.927484: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-12 01:29:09.927498: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-12 01:29:09.932115: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import pprint\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from sac import Agent\n",
    "from obs import observation_shape\n",
    "from utils import record_videos, load_config\n",
    "\n",
    "filename = 'inverted_pendulum.png'\n",
    "\n",
    "best_score = -10000.0\n",
    "best_score = -1000.0\n",
    "score_history = []\n",
    "episode_lens = []\n",
    "avg_history = []\n",
    "std_history = []\n",
    "avg_history_100 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': {'lateral': True, 'longitudinal': True, 'type': 'ContinuousAction'},\n",
      " 'action_reward': -0.3,\n",
      " 'centering_position': [0.7, 0.7],\n",
      " 'collision_reward': -1,\n",
      " 'controlled_vehicles': 1,\n",
      " 'duration': 80,\n",
      " 'lane_centering_cost': 4,\n",
      " 'lane_centering_reward': 1,\n",
      " 'manual_control': False,\n",
      " 'observation': {'features': ['presence',\n",
      "                              'x',\n",
      "                              'y',\n",
      "                              'vx',\n",
      "                              'vy',\n",
      "                              'cos_h',\n",
      "                              'sin_h',\n",
      "                              'heading',\n",
      "                              'long_off',\n",
      "                              'lat_off',\n",
      "                              'ang_off'],\n",
      "                 'features_range': {'vx': [-20, 20],\n",
      "                                    'vy': [-20, 20],\n",
      "                                    'x': [-100, 100],\n",
      "                                    'y': [-100, 100]},\n",
      "                 'type': 'Kinematics',\n",
      "                 'vehicles_count': 1},\n",
      " 'offscreen_rendering': False,\n",
      " 'other_vehicles': 1,\n",
      " 'other_vehicles_type': 'highway_env.vehicle.behavior.IDMVehicle',\n",
      " 'policy_frequency': 5,\n",
      " 'real_time_rendering': True,\n",
      " 'render_agent': True,\n",
      " 'scaling': 5.5,\n",
      " 'screen_height': 600,\n",
      " 'screen_width': 600,\n",
      " 'show_trajectories': True,\n",
      " 'simulation_frequency': 15}\n",
      "(26,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/o/Documents/thesis/.venv/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.configure to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.configure` for environment variables or `env.get_wrapper_attr('configure')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/o/Documents/thesis/.venv/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.config to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.config` for environment variables or `env.get_wrapper_attr('config')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "data = load_config()\n",
    "env = gym.make('racetrack-v0', render_mode = 'rgb_array')\n",
    "env.configure(data) # type: ignore\n",
    "pprint.pprint(env.config) # type: ignore\n",
    "(obs, info), done = env.reset(), False\n",
    "\n",
    "#observation config\n",
    "proc = observation_shape(obs,info,2)\n",
    "proc.reset()\n",
    "input = proc.get_input()\n",
    "print(input.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 01:29:32.073701: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-12 01:29:32.094768: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-12 01:29:32.094876: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-12 01:29:32.095523: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-12 01:29:32.095603: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-12 01:29:32.095650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-12 01:29:32.424764: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-12 01:29:32.424947: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-12 01:29:32.425018: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-12 01:29:32.425067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9792 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(input_dims=input.shape, env=env,\n",
    "            n_actions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196a5dac88b945e1954e7f5e0ff9ad2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test episodes:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 00:44:37.978346: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  1 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  2 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  3 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  4 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  5 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  6 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  7 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  8 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  9 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  10 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  11 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  12 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  13 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  14 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  15 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  16 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  17 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  18 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  19 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  20 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  21 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  22 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  23 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  24 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  25 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  26 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  27 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  28 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  29 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  30 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  31 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  32 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  33 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  34 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  35 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  36 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  37 score -0.3 ep len 3 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  38 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  39 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  40 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  41 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  42 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  43 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  44 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  45 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  46 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  47 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n",
      "episode  48 score -0.1 ep len 2 avg score -0.1 avg_score_100 -0.1 std score 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 00:44:41.051662: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556c88c8db30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-12 00:44:41.051680: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2023-12-12 00:44:41.054371: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-12-12 00:44:42.018825: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8902\n",
      "2023-12-12 00:44:42.072867: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f2a2bfd9120> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f2a2bfd9120> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "episode  49 score 0.6 ep len 7 avg score -0.1 avg_score_100 -0.1 std score 0.1\n",
      "episode  50 score 0.6 ep len 7 avg score -0.1 avg_score_100 -0.1 std score 0.1\n",
      "episode  51 score 0.8 ep len 7 avg score -0.1 avg_score_100 -0.1 std score 0.2\n",
      "episode  52 score 0.8 ep len 14 avg score -0.1 avg_score_100 -0.1 std score 0.2\n",
      "episode  53 score 2.5 ep len 28 avg score -0.0 avg_score_100 -0.0 std score 0.4\n",
      "episode  54 score -0.3 ep len 6 avg score -0.0 avg_score_100 -0.0 std score 0.4\n",
      "episode  55 score -0.2 ep len 7 avg score -0.0 avg_score_100 -0.0 std score 0.4\n",
      "episode  56 score 1.8 ep len 9 avg score 0.0 avg_score_100 0.0 std score 0.5\n",
      "episode  57 score 0.2 ep len 7 avg score 0.0 avg_score_100 0.0 std score 0.5\n",
      "episode  58 score -0.9 ep len 7 avg score -0.0 avg_score_100 -0.0 std score 0.5\n",
      "episode  59 score 0.8 ep len 8 avg score 0.0 avg_score_100 0.0 std score 0.5\n",
      "episode  60 score 8.0 ep len 44 avg score 0.1 avg_score_100 0.1 std score 1.1\n",
      "episode  61 score 0.5 ep len 7 avg score 0.1 avg_score_100 0.1 std score 1.1\n",
      "episode  62 score 2.4 ep len 7 avg score 0.2 avg_score_100 0.2 std score 1.1\n",
      "episode  63 score 2.2 ep len 14 avg score 0.2 avg_score_100 0.2 std score 1.2\n",
      "episode  64 score 0.3 ep len 4 avg score 0.2 avg_score_100 0.2 std score 1.2\n",
      "episode  65 score -0.0 ep len 6 avg score 0.2 avg_score_100 0.2 std score 1.1\n",
      "episode  66 score 0.3 ep len 6 avg score 0.2 avg_score_100 0.2 std score 1.1\n",
      "episode  67 score 0.3 ep len 5 avg score 0.2 avg_score_100 0.2 std score 1.1\n",
      "episode  68 score -0.3 ep len 3 avg score 0.2 avg_score_100 0.2 std score 1.1\n",
      "episode  69 score 1.7 ep len 10 avg score 0.2 avg_score_100 0.2 std score 1.1\n",
      "episode  70 score 4.2 ep len 40 avg score 0.3 avg_score_100 0.3 std score 1.2\n",
      "episode  71 score 2.6 ep len 14 avg score 0.3 avg_score_100 0.3 std score 1.2\n",
      "episode  72 score 2.8 ep len 12 avg score 0.3 avg_score_100 0.3 std score 1.3\n",
      "episode  73 score 0.5 ep len 11 avg score 0.3 avg_score_100 0.3 std score 1.3\n",
      "episode  74 score 6.5 ep len 29 avg score 0.4 avg_score_100 0.4 std score 1.4\n",
      "episode  75 score 2.4 ep len 24 avg score 0.5 avg_score_100 0.5 std score 1.4\n",
      "episode  76 score 3.4 ep len 13 avg score 0.5 avg_score_100 0.5 std score 1.5\n",
      "episode  77 score 1.4 ep len 10 avg score 0.5 avg_score_100 0.5 std score 1.5\n",
      "episode  78 score 2.6 ep len 9 avg score 0.5 avg_score_100 0.5 std score 1.5\n",
      "episode  79 score 9.7 ep len 57 avg score 0.6 avg_score_100 0.6 std score 1.8\n",
      "episode  80 score 0.3 ep len 7 avg score 0.6 avg_score_100 0.6 std score 1.8\n",
      "episode  81 score 9.0 ep len 54 avg score 0.7 avg_score_100 0.7 std score 2.0\n",
      "episode  82 score 4.4 ep len 21 avg score 0.8 avg_score_100 0.8 std score 2.0\n",
      "episode  83 score 5.3 ep len 27 avg score 0.8 avg_score_100 0.8 std score 2.1\n",
      "episode  84 score 19.6 ep len 61 avg score 1.1 avg_score_100 1.1 std score 2.9\n",
      "episode  85 score 16.6 ep len 73 avg score 1.2 avg_score_100 1.2 std score 3.3\n",
      "episode  86 score 21.2 ep len 91 avg score 1.5 avg_score_100 1.5 std score 3.9\n",
      "episode  87 score 42.1 ep len 167 avg score 1.9 avg_score_100 1.9 std score 5.8\n",
      "episode  88 score 48.4 ep len 141 avg score 2.5 avg_score_100 2.5 std score 7.6\n",
      "episode  89 score 60.4 ep len 200 avg score 3.1 avg_score_100 3.1 std score 9.7\n",
      "episode  90 score 72.5 ep len 200 avg score 3.9 avg_score_100 3.9 std score 12.0\n",
      "episode  91 score 78.6 ep len 200 avg score 4.7 avg_score_100 4.7 std score 14.3\n",
      "episode  92 score 79.3 ep len 200 avg score 5.5 avg_score_100 5.5 std score 16.1\n",
      "episode  93 score 79.2 ep len 200 avg score 6.3 avg_score_100 6.3 std score 17.7\n",
      "episode  94 score 66.6 ep len 200 avg score 6.9 avg_score_100 6.9 std score 18.7\n",
      "episode  95 score 73.8 ep len 200 avg score 7.6 avg_score_100 7.6 std score 19.8\n",
      "episode  96 score 77.5 ep len 200 avg score 8.3 avg_score_100 8.3 std score 20.9\n",
      "episode  97 score 72.7 ep len 200 avg score 9.0 avg_score_100 9.0 std score 21.8\n",
      "episode  98 score 142.2 ep len 200 avg score 10.3 avg_score_100 10.3 std score 25.5\n",
      "episode  99 score 163.8 ep len 200 avg score 11.9 avg_score_100 11.9 std score 29.6\n",
      "episode  100 score 165.2 ep len 200 avg score 13.4 avg_score_100 13.5 std score 33.1\n",
      "episode  101 score 167.4 ep len 200 avg score 14.9 avg_score_100 15.2 std score 36.3\n",
      "episode  102 score 165.9 ep len 200 avg score 16.3 avg_score_100 16.8 std score 39.0\n",
      "episode  103 score 168.5 ep len 200 avg score 17.8 avg_score_100 18.5 std score 41.6\n",
      "episode  104 score 170.0 ep len 200 avg score 19.3 avg_score_100 20.2 std score 43.9\n",
      "episode  105 score 170.0 ep len 200 avg score 20.7 avg_score_100 21.9 std score 46.1\n",
      "episode  106 score 172.4 ep len 200 avg score 22.1 avg_score_100 23.7 std score 48.1\n",
      "episode  107 score 168.3 ep len 200 avg score 23.5 avg_score_100 25.3 std score 49.9\n",
      "episode  108 score 171.6 ep len 200 avg score 24.8 avg_score_100 27.1 std score 51.7\n",
      "episode  109 score 173.8 ep len 200 avg score 26.2 avg_score_100 28.8 std score 53.3\n",
      "episode  110 score 170.8 ep len 200 avg score 27.5 avg_score_100 30.5 std score 54.8\n",
      "episode  111 score 170.9 ep len 200 avg score 28.8 avg_score_100 32.2 std score 56.2\n",
      "episode  112 score 171.7 ep len 200 avg score 30.0 avg_score_100 33.9 std score 57.6\n",
      "episode  113 score 164.2 ep len 200 avg score 31.2 avg_score_100 35.6 std score 58.7\n",
      "episode  114 score 173.3 ep len 200 avg score 32.4 avg_score_100 37.3 std score 59.9\n",
      "episode  115 score 171.7 ep len 200 avg score 33.6 avg_score_100 39.0 std score 61.0\n",
      "episode  116 score 170.4 ep len 200 avg score 34.8 avg_score_100 40.7 std score 62.0\n",
      "episode  117 score 172.3 ep len 200 avg score 36.0 avg_score_100 42.5 std score 63.0\n",
      "episode  118 score 173.5 ep len 200 avg score 37.1 avg_score_100 44.2 std score 64.0\n",
      "episode  119 score 172.8 ep len 200 avg score 38.3 avg_score_100 45.9 std score 64.9\n",
      "episode  120 score 172.0 ep len 200 avg score 39.4 avg_score_100 47.6 std score 65.8\n",
      "episode  121 score 175.9 ep len 200 avg score 40.5 avg_score_100 49.4 std score 66.7\n",
      "episode  122 score 174.2 ep len 200 avg score 41.6 avg_score_100 51.2 std score 67.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m         truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     agent\u001b[38;5;241m.\u001b[39mremember(state\u001b[38;5;241m=\u001b[39mobservation, action\u001b[38;5;241m=\u001b[39maction, done\u001b[38;5;241m=\u001b[39mdone,\n\u001b[1;32m     28\u001b[0m                     reward\u001b[38;5;241m=\u001b[39mreward, new_state\u001b[38;5;241m=\u001b[39mnew_observation)\n\u001b[0;32m---> 29\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     observation \u001b[38;5;241m=\u001b[39m new_observation\n\u001b[1;32m     33\u001b[0m episode_lens\u001b[38;5;241m.\u001b[39mappend(episode_len)\n",
      "File \u001b[0;32m~/Documents/thesis/SAC/sac.py:250\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_1\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    246\u001b[0m     critic_1_network_gradient, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_1\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_2\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    248\u001b[0m     critic_2_network_gradient, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_2\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_network_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m    253\u001b[0m K\u001b[38;5;241m.\u001b[39mclear_session()\n",
      "File \u001b[0;32m~/Documents/thesis/SAC/sac.py:159\u001b[0m, in \u001b[0;36mAgent.update_network_parameters\u001b[0;34m(self, tau)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mweights):\n\u001b[1;32m    157\u001b[0m     weights\u001b[38;5;241m.\u001b[39mappend(weight \u001b[38;5;241m*\u001b[39m tau \u001b[38;5;241m+\u001b[39m targets[i]\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mtau))\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/keras/src/engine/base_layer.py:1830\u001b[0m, in \u001b[0;36mLayer.set_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1827\u001b[0m         weight_value_tuples\u001b[38;5;241m.\u001b[39mappend((param, weight))\n\u001b[1;32m   1828\u001b[0m         weight_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1830\u001b[0m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_set_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_value_tuples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1832\u001b[0m \u001b[38;5;66;03m# Perform any layer defined finalization of the layer state.\u001b[39;00m\n\u001b[1;32m   1833\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flatten_layers():\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/keras/src/backend.py:4313\u001b[0m, in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   4311\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, value \u001b[38;5;129;01min\u001b[39;00m tuples:\n\u001b[1;32m   4312\u001b[0m         value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(value, dtype\u001b[38;5;241m=\u001b[39mdtype_numpy(x))\n\u001b[0;32m-> 4313\u001b[0m         \u001b[43m_assign_value_to_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4315\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_graph()\u001b[38;5;241m.\u001b[39mas_default():\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/keras/src/backend.py:4361\u001b[0m, in \u001b[0;36m_assign_value_to_variable\u001b[0;34m(variable, value)\u001b[0m\n\u001b[1;32m   4358\u001b[0m     variable\u001b[38;5;241m.\u001b[39massign(d_value)\n\u001b[1;32m   4359\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4360\u001b[0m     \u001b[38;5;66;03m# For the normal tf.Variable assign\u001b[39;00m\n\u001b[0;32m-> 4361\u001b[0m     \u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py:1031\u001b[0m, in \u001b[0;36mBaseResourceVariable.assign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m   1029\u001b[0m   validate_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_shape \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape\u001b[38;5;241m.\u001b[39mis_fully_defined()\n\u001b[1;32m   1030\u001b[0m   kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidate_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m validate_shape\n\u001b[0;32m-> 1031\u001b[0m assign_op \u001b[38;5;241m=\u001b[39m \u001b[43mgen_resource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_variable_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_value:\n\u001b[1;32m   1034\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_read(assign_op)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py:148\u001b[0m, in \u001b[0;36massign_variable_op\u001b[0;34m(resource, value, validate_shape, name)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m    147\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAssignVariableOp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidate_shape\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalidate_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m    152\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main loop\n",
    "for episode in trange(150, desc='Test episodes'):\n",
    "\n",
    "        proc.reset()        \n",
    "        (observation, info), done = env.reset(), False\n",
    "        truncated = False\n",
    "        \n",
    "        proc.update_input(observation, info)\n",
    "        observation = proc.get_input()\n",
    "\n",
    "        episode_reward = 0\n",
    "        episode_len = 0\n",
    "\n",
    "        while (not done) and (not truncated):\n",
    "            action = agent.choose_action(observation)\n",
    "\n",
    "            new_observation, reward, done, truncated, new_info = env.step(action=action)\n",
    "            proc.update_input(new_observation, info)\n",
    "            new_observation = proc.get_input()\n",
    "\n",
    "            episode_reward += reward # type: ignore\n",
    "            episode_len +=1\n",
    "\n",
    "            if new_info[\"rewards\"][\"on_road_reward\"] == False or episode_len >= 200:\n",
    "                truncated = True\n",
    "\n",
    "            agent.remember(state=observation, action=action, done=done,\n",
    "                            reward=reward, new_state=new_observation)\n",
    "            agent.learn()\n",
    "\n",
    "            observation = new_observation\n",
    "\n",
    "        episode_lens.append(episode_len)\n",
    "\n",
    "        score_history.append(episode_reward)\n",
    "        avg_score = np.mean(score_history)\n",
    "        avg_history.append(avg_score)\n",
    "        std_score = np.std(score_history)\n",
    "        std_history.append(std_score)\n",
    "\n",
    "        avg_score_100 = np.mean(score_history[-100:])\n",
    "        avg_history_100.append(avg_score_100)\n",
    "\n",
    "        agent.tensorboard.update_stats(episode_rew = episode_reward, \n",
    "                                       average_rew =avg_score,\n",
    "                                       average_100_reward = avg_score_100,\n",
    "                                       std_rew=std_score, \n",
    "                                       episode_len = episode_len)\n",
    "\n",
    "        print('episode ', episode, 'score %.1f' % episode_reward, 'ep len', episode_len,\n",
    "              'avg score %.1f' % avg_score, 'avg_score_100 %.1f' %avg_score_100,'std score %.1f' % std_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7973\n"
     ]
    }
   ],
   "source": [
    "n_steps = sum(episode_lens)\n",
    "print(n_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/o/Documents/thesis/.venv/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.configure to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.configure` for environment variables or `env.get_wrapper_attr('configure')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/o/Documents/thesis/.venv/lib/python3.10/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/o/Documents/thesis/SAC/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7edd260b0b5d445796ab994ce8a33fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test episodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score 172.8 ep len 201\n",
      "Moviepy - Building video /home/o/Documents/thesis/SAC/videos/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/o/Documents/thesis/SAC/videos/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/o/Documents/thesis/SAC/videos/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "# env = record_videos(env)\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "data = load_config()\n",
    "env = gym.make('racetrack-v0', render_mode = 'rgb_array')\n",
    "env.configure(data) # type: ignore\n",
    "\n",
    "env = RecordVideo(env, video_folder=\"videos\", episode_trigger=lambda e: True)\n",
    "env.unwrapped.set_record_video_wrapper(env)\n",
    "\n",
    "\n",
    "for episode in trange(1, desc='Test episodes'):\n",
    "\n",
    "        proc.reset()        \n",
    "        (observation, info), done = env.reset(), False\n",
    "        truncated = False\n",
    "        \n",
    "        proc.update_input(observation, info)\n",
    "        observation = proc.get_input()\n",
    "\n",
    "        episode_reward = 0\n",
    "        episode_len = 0\n",
    "\n",
    "        while (not done) and (not truncated):\n",
    "            action = agent.choose_action(observation)\n",
    "\n",
    "            new_observation, reward, done, truncated, new_info = env.step(action=[action])\n",
    "            proc.update_input(new_observation, info)\n",
    "            new_observation = proc.get_input()\n",
    "            \n",
    "            if new_info[\"rewards\"][\"on_road_reward\"] == False or episode_len >= 200:\n",
    "                truncated = True\n",
    "\n",
    "\n",
    "            episode_reward += reward # type: ignore\n",
    "            episode_len +=1\n",
    "\n",
    "            observation = new_observation\n",
    "\n",
    "        print('episode ', episode, 'score %.1f' % episode_reward, 'ep len', episode_len)\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
