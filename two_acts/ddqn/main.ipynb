{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 23:43:26.031633: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-11 23:43:26.054857: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-11 23:43:26.054879: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-11 23:43:26.054892: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-11 23:43:26.059589: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# fundemental modules\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import pprint\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from ddqn import DDQNAgent # type: ignore\n",
    "from utils import load_config\n",
    "from obs import observation_shape\n",
    "\n",
    "aggregate_stats_every=100\n",
    "best_score = -1000.0\n",
    "score_history = []\n",
    "episode_lens = []\n",
    "avg_history = []\n",
    "std_history = []\n",
    "avg_history_100 = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/o/Documents/thesis/.venv/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.configure to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.configure` for environment variables or `env.get_wrapper_attr('configure')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "data = load_config()\n",
    "env = gym.make('racetrack-v0', render_mode = 'rgb_array')\n",
    "env.configure(data) # type: ignore\n",
    "# pprint.pprint(env.config) # type: ignore\n",
    "(obs, info), done = env.reset(), False\n",
    "\n",
    "#observation config\n",
    "proc = observation_shape(obs,info,2)\n",
    "proc.reset()\n",
    "input = proc.get_input()\n",
    "print(input.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               3456      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 11)                1419      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21387 (83.54 KB)\n",
      "Trainable params: 21387 (83.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "summary\n",
      "Agent is initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 23:43:30.903474: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-11 23:43:31.022182: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-11 23:43:31.022513: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-11 23:43:31.025026: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-11 23:43:31.025288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-11 23:43:31.025444: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-11 23:43:31.594265: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-11 23:43:31.594358: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-11 23:43:31.594409: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-11 23:43:31.594450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9928 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "agent = DDQNAgent(alpha=0.001, gamma=0.9, epsilon=1.0, obs_shape=input.shape,\n",
    "                  batch_size=64, epsilon_dec=0.995, epsilon_end=0.05, mem_size=100000,\n",
    "                  min_mem_size=100, replace_target=100, learning_rate=0.0003)\n",
    "\n",
    "\n",
    "# agent = DDQNAgent(alpha=0.001, gamma=0.9, epsilon=1.0, obs_shape=input.shape,\n",
    "#                   batch_size=64, epsilon_dec=0.9995, epsilon_end=0.05, mem_size=100000,\n",
    "#                   min_mem_size=100, replace_target=1000, learning_rate=0.0003)\n",
    "print(\"Agent is initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82010dd48d64f6e8bcd8c681d215536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test episodes:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----saving models------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 23:43:43.552670: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score 1.5 ep len 23 avg score 1.5 avg_score_100 1.5 std score 0.0\n",
      "-----saving models------\n",
      "episode  1 score 3.2 ep len 13 avg score 2.4 avg_score_100 2.4 std score 0.8\n",
      "episode  2 score 0.2 ep len 4 avg score 1.7 avg_score_100 1.7 std score 1.2\n",
      "episode  3 score 3.1 ep len 16 avg score 2.0 avg_score_100 2.0 std score 1.2\n",
      "episode  4 score 0.8 ep len 6 avg score 1.8 avg_score_100 1.8 std score 1.2\n",
      "episode  5 score 2.1 ep len 9 avg score 1.8 avg_score_100 1.8 std score 1.1\n",
      "episode  6 score -0.0 ep len 5 avg score 1.6 avg_score_100 1.6 std score 1.2\n",
      "episode  7 score 1.6 ep len 5 avg score 1.6 avg_score_100 1.6 std score 1.1\n",
      "episode  8 score 0.4 ep len 11 avg score 1.4 avg_score_100 1.4 std score 1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 23:43:45.858171: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55bf06c94230 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-11 23:43:45.858184: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2023-12-11 23:43:45.863433: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-12-11 23:43:47.121556: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8902\n",
      "2023-12-11 23:43:47.183225: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Updated\n",
      "episode  9 score 2.0 ep len 16 avg score 1.5 avg_score_100 1.5 std score 1.1\n",
      "episode  10 score 0.4 ep len 13 avg score 1.4 avg_score_100 1.4 std score 1.1\n",
      "episode  11 score -0.2 ep len 4 avg score 1.3 avg_score_100 1.3 std score 1.1\n",
      "episode  12 score 1.0 ep len 5 avg score 1.2 avg_score_100 1.2 std score 1.1\n",
      "episode  13 score 2.3 ep len 12 avg score 1.3 avg_score_100 1.3 std score 1.1\n",
      "episode  14 score 3.0 ep len 8 avg score 1.4 avg_score_100 1.4 std score 1.1\n",
      "episode  15 score 0.8 ep len 12 avg score 1.4 avg_score_100 1.4 std score 1.1\n",
      "episode  16 score 1.5 ep len 7 avg score 1.4 avg_score_100 1.4 std score 1.1\n",
      "episode  17 score 1.1 ep len 9 avg score 1.4 avg_score_100 1.4 std score 1.0\n",
      "episode  18 score 1.8 ep len 10 avg score 1.4 avg_score_100 1.4 std score 1.0\n",
      "episode  19 score 0.9 ep len 7 avg score 1.4 avg_score_100 1.4 std score 1.0\n",
      "episode  20 score -0.2 ep len 3 avg score 1.3 avg_score_100 1.3 std score 1.0\n",
      "Target Updated\n",
      "episode  21 score 1.1 ep len 9 avg score 1.3 avg_score_100 1.3 std score 1.0\n",
      "episode  22 score 1.7 ep len 5 avg score 1.3 avg_score_100 1.3 std score 1.0\n",
      "episode  23 score 0.6 ep len 6 avg score 1.3 avg_score_100 1.3 std score 1.0\n",
      "episode  24 score 0.9 ep len 6 avg score 1.3 avg_score_100 1.3 std score 1.0\n",
      "episode  25 score -0.2 ep len 3 avg score 1.2 avg_score_100 1.2 std score 1.0\n",
      "episode  26 score 0.6 ep len 12 avg score 1.2 avg_score_100 1.2 std score 1.0\n",
      "episode  27 score -0.4 ep len 9 avg score 1.1 avg_score_100 1.1 std score 1.0\n",
      "episode  28 score 2.4 ep len 26 avg score 1.2 avg_score_100 1.2 std score 1.0\n",
      "episode  29 score 1.1 ep len 9 avg score 1.2 avg_score_100 1.2 std score 1.0\n",
      "episode  30 score 1.4 ep len 7 avg score 1.2 avg_score_100 1.2 std score 1.0\n",
      "episode  31 score 3.2 ep len 8 avg score 1.2 avg_score_100 1.2 std score 1.0\n",
      "Target Updated\n",
      "episode  32 score 0.9 ep len 4 avg score 1.2 avg_score_100 1.2 std score 1.0\n",
      "episode  33 score 1.2 ep len 5 avg score 1.2 avg_score_100 1.2 std score 1.0\n",
      "episode  34 score 3.1 ep len 30 avg score 1.3 avg_score_100 1.3 std score 1.0\n",
      "episode  35 score 7.6 ep len 11 avg score 1.5 avg_score_100 1.5 std score 1.4\n",
      "episode  36 score 10.5 ep len 30 avg score 1.7 avg_score_100 1.7 std score 2.0\n",
      "episode  37 score 4.3 ep len 12 avg score 1.8 avg_score_100 1.8 std score 2.1\n",
      "Target Updated\n",
      "episode  38 score 2.2 ep len 13 avg score 1.8 avg_score_100 1.8 std score 2.0\n",
      "episode  39 score 15.6 ep len 29 avg score 2.1 avg_score_100 2.1 std score 2.9\n",
      "episode  40 score 5.5 ep len 17 avg score 2.2 avg_score_100 2.2 std score 3.0\n",
      "episode  41 score 0.2 ep len 5 avg score 2.2 avg_score_100 2.2 std score 2.9\n",
      "episode  42 score 4.2 ep len 9 avg score 2.2 avg_score_100 2.2 std score 2.9\n",
      "episode  43 score 2.6 ep len 8 avg score 2.2 avg_score_100 2.2 std score 2.9\n",
      "episode  44 score 3.6 ep len 12 avg score 2.3 avg_score_100 2.3 std score 2.9\n",
      "-----saving models------\n",
      "episode  45 score 9.7 ep len 14 avg score 2.4 avg_score_100 2.4 std score 3.0\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  46 score 7.1 ep len 21 avg score 2.5 avg_score_100 2.5 std score 3.1\n",
      "-----saving models------\n",
      "episode  47 score 10.7 ep len 18 avg score 2.7 avg_score_100 2.7 std score 3.3\n",
      "-----saving models------\n",
      "episode  48 score 12.9 ep len 23 avg score 2.9 avg_score_100 2.9 std score 3.5\n",
      "-----saving models------\n",
      "episode  49 score 12.4 ep len 23 avg score 3.1 avg_score_100 3.1 std score 3.7\n",
      "-----saving models------\n",
      "episode  50 score 10.7 ep len 15 avg score 3.2 avg_score_100 3.2 std score 3.9\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  51 score 5.9 ep len 11 avg score 3.3 avg_score_100 3.3 std score 3.8\n",
      "-----saving models------\n",
      "episode  52 score 11.7 ep len 24 avg score 3.4 avg_score_100 3.4 std score 4.0\n",
      "-----saving models------\n",
      "episode  53 score 12.9 ep len 17 avg score 3.6 avg_score_100 3.6 std score 4.1\n",
      "-----saving models------\n",
      "episode  54 score 20.3 ep len 30 avg score 3.9 avg_score_100 3.9 std score 4.7\n",
      "episode  55 score 3.7 ep len 13 avg score 3.9 avg_score_100 3.9 std score 4.6\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  56 score 9.3 ep len 21 avg score 4.0 avg_score_100 4.0 std score 4.6\n",
      "-----saving models------\n",
      "episode  57 score 11.1 ep len 18 avg score 4.1 avg_score_100 4.1 std score 4.7\n",
      "-----saving models------\n",
      "episode  58 score 8.1 ep len 15 avg score 4.2 avg_score_100 4.2 std score 4.7\n",
      "-----saving models------\n",
      "episode  59 score 6.9 ep len 24 avg score 4.2 avg_score_100 4.2 std score 4.7\n",
      "-----saving models------\n",
      "episode  60 score 11.5 ep len 21 avg score 4.4 avg_score_100 4.4 std score 4.7\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  61 score 8.0 ep len 25 avg score 4.4 avg_score_100 4.4 std score 4.7\n",
      "-----saving models------\n",
      "episode  62 score 17.1 ep len 28 avg score 4.6 avg_score_100 4.6 std score 4.9\n",
      "-----saving models------\n",
      "episode  63 score 15.5 ep len 30 avg score 4.8 avg_score_100 4.8 std score 5.1\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  64 score 15.8 ep len 35 avg score 5.0 avg_score_100 5.0 std score 5.2\n",
      "-----saving models------\n",
      "episode  65 score 13.9 ep len 16 avg score 5.1 avg_score_100 5.1 std score 5.3\n",
      "-----saving models------\n",
      "episode  66 score 18.4 ep len 32 avg score 5.3 avg_score_100 5.3 std score 5.5\n",
      "-----saving models------\n",
      "episode  67 score 21.9 ep len 35 avg score 5.5 avg_score_100 5.5 std score 5.8\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  68 score 25.2 ep len 32 avg score 5.8 avg_score_100 5.8 std score 6.2\n",
      "-----saving models------\n",
      "episode  69 score 26.5 ep len 42 avg score 6.1 avg_score_100 6.1 std score 6.6\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  70 score 22.0 ep len 44 avg score 6.3 avg_score_100 6.3 std score 6.8\n",
      "-----saving models------\n",
      "episode  71 score 14.3 ep len 42 avg score 6.5 avg_score_100 6.5 std score 6.9\n",
      "-----saving models------\n",
      "episode  72 score 14.9 ep len 46 avg score 6.6 avg_score_100 6.6 std score 6.9\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  73 score 23.7 ep len 48 avg score 6.8 avg_score_100 6.8 std score 7.1\n",
      "episode  74 score 6.7 ep len 42 avg score 6.8 avg_score_100 6.8 std score 7.1\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  75 score 24.9 ep len 52 avg score 7.0 avg_score_100 7.0 std score 7.3\n",
      "-----saving models------\n",
      "episode  76 score 12.9 ep len 43 avg score 7.1 avg_score_100 7.1 std score 7.3\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  77 score 9.9 ep len 38 avg score 7.2 avg_score_100 7.2 std score 7.3\n",
      "-----saving models------\n",
      "episode  78 score 17.3 ep len 55 avg score 7.3 avg_score_100 7.3 std score 7.3\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  79 score 18.2 ep len 65 avg score 7.4 avg_score_100 7.4 std score 7.4\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  80 score 18.6 ep len 70 avg score 7.6 avg_score_100 7.6 std score 7.4\n",
      "-----saving models------\n",
      "episode  81 score 27.2 ep len 58 avg score 7.8 avg_score_100 7.8 std score 7.7\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  82 score 29.8 ep len 72 avg score 8.1 avg_score_100 8.1 std score 8.0\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  83 score 45.8 ep len 141 avg score 8.5 avg_score_100 8.5 std score 8.9\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  84 score 18.2 ep len 71 avg score 8.6 avg_score_100 8.6 std score 9.0\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  85 score 30.3 ep len 82 avg score 8.9 avg_score_100 8.9 std score 9.2\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  86 score 15.9 ep len 78 avg score 9.0 avg_score_100 9.0 std score 9.2\n",
      "-----saving models------\n",
      "episode  87 score 40.5 ep len 86 avg score 9.3 avg_score_100 9.3 std score 9.7\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  88 score 27.6 ep len 67 avg score 9.5 avg_score_100 9.5 std score 9.9\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  89 score 37.4 ep len 84 avg score 9.8 avg_score_100 9.8 std score 10.2\n",
      "-----saving models------\n",
      "episode  90 score 28.5 ep len 43 avg score 10.0 avg_score_100 10.0 std score 10.4\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  91 score 38.0 ep len 85 avg score 10.3 avg_score_100 10.3 std score 10.7\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  92 score 39.6 ep len 81 avg score 10.7 avg_score_100 10.7 std score 11.1\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  93 score 30.0 ep len 66 avg score 10.9 avg_score_100 10.9 std score 11.2\n",
      "-----saving models------\n",
      "episode  94 score 26.0 ep len 47 avg score 11.0 avg_score_100 11.0 std score 11.2\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  95 score 36.3 ep len 90 avg score 11.3 avg_score_100 11.3 std score 11.5\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  96 score 46.3 ep len 165 avg score 11.6 avg_score_100 11.6 std score 11.9\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  97 score 27.3 ep len 90 avg score 11.8 avg_score_100 11.8 std score 12.0\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  98 score 73.7 ep len 169 avg score 12.4 avg_score_100 12.4 std score 13.4\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  99 score 60.1 ep len 144 avg score 12.9 avg_score_100 12.9 std score 14.2\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  100 score 73.2 ep len 142 avg score 13.5 avg_score_100 13.6 std score 15.3\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  101 score 39.6 ep len 66 avg score 13.8 avg_score_100 14.0 std score 15.5\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  102 score 85.4 ep len 200 avg score 14.5 avg_score_100 14.8 std score 16.9\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  103 score 76.0 ep len 175 avg score 15.0 avg_score_100 15.6 std score 17.9\n",
      "-----saving models------\n",
      "episode  104 score 21.8 ep len 47 avg score 15.1 avg_score_100 15.8 std score 17.8\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  105 score 74.4 ep len 149 avg score 15.7 avg_score_100 16.5 std score 18.6\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  106 score 105.6 ep len 174 avg score 16.5 avg_score_100 17.6 std score 20.5\n",
      "-----saving models------\n",
      "episode  107 score 7.8 ep len 14 avg score 16.4 avg_score_100 17.6 std score 20.4\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  108 score 67.2 ep len 149 avg score 16.9 avg_score_100 18.3 std score 20.9\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  109 score 71.9 ep len 129 avg score 17.4 avg_score_100 19.0 std score 21.4\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  110 score 83.8 ep len 155 avg score 18.0 avg_score_100 19.8 std score 22.2\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  111 score 102.2 ep len 200 avg score 18.7 avg_score_100 20.8 std score 23.5\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  112 score 91.0 ep len 176 avg score 19.4 avg_score_100 21.7 std score 24.3\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  113 score 110.0 ep len 196 avg score 20.2 avg_score_100 22.8 std score 25.7\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  114 score 112.8 ep len 200 avg score 21.0 avg_score_100 23.9 std score 27.0\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  115 score 97.8 ep len 181 avg score 21.6 avg_score_100 24.9 std score 27.8\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  116 score 124.6 ep len 200 avg score 22.5 avg_score_100 26.1 std score 29.2\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  117 score 122.7 ep len 200 avg score 23.4 avg_score_100 27.3 std score 30.5\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  118 score 111.6 ep len 200 avg score 24.1 avg_score_100 28.4 std score 31.4\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  119 score 117.0 ep len 200 avg score 24.9 avg_score_100 29.6 std score 32.4\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  120 score 114.0 ep len 200 avg score 25.6 avg_score_100 30.7 std score 33.3\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  121 score 119.6 ep len 200 avg score 26.4 avg_score_100 31.9 std score 34.2\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  122 score 137.8 ep len 200 avg score 27.3 avg_score_100 33.3 std score 35.5\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  123 score 126.7 ep len 200 avg score 28.1 avg_score_100 34.5 std score 36.5\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  124 score 135.5 ep len 200 avg score 29.0 avg_score_100 35.9 std score 37.6\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  125 score 122.2 ep len 200 avg score 29.7 avg_score_100 37.1 std score 38.3\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  126 score 127.8 ep len 200 avg score 30.5 avg_score_100 38.4 std score 39.1\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  127 score 136.1 ep len 200 avg score 31.3 avg_score_100 39.8 std score 40.1\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  128 score 129.9 ep len 200 avg score 32.1 avg_score_100 41.0 std score 40.9\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  129 score 134.2 ep len 200 avg score 32.9 avg_score_100 42.4 std score 41.7\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  130 score 123.6 ep len 200 avg score 33.5 avg_score_100 43.6 std score 42.3\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  131 score 132.6 ep len 200 avg score 34.3 avg_score_100 44.9 std score 43.0\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  132 score 127.4 ep len 200 avg score 35.0 avg_score_100 46.1 std score 43.5\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  133 score 140.9 ep len 200 avg score 35.8 avg_score_100 47.5 std score 44.3\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  134 score 149.8 ep len 200 avg score 36.6 avg_score_100 49.0 std score 45.2\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  135 score 90.5 ep len 131 avg score 37.0 avg_score_100 49.8 std score 45.3\n",
      "Target Updated\n",
      "Target Updated\n",
      "-----saving models------\n",
      "episode  136 score 135.0 ep len 200 avg score 37.7 avg_score_100 51.1 std score 45.9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m         truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     agent\u001b[38;5;241m.\u001b[39mremember(state\u001b[38;5;241m=\u001b[39mobservation, action\u001b[38;5;241m=\u001b[39maction_index, done\u001b[38;5;241m=\u001b[39mdone,\n\u001b[1;32m     28\u001b[0m                     reward\u001b[38;5;241m=\u001b[39mreward, new_state\u001b[38;5;241m=\u001b[39mnew_observation)\n\u001b[0;32m---> 29\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     observation \u001b[38;5;241m=\u001b[39m new_observation\n\u001b[1;32m     33\u001b[0m episode_lens\u001b[38;5;241m.\u001b[39mappend(episode_len)\n",
      "File \u001b[0;32m~/Documents/thesis/ddqn/ddqn.py:141\u001b[0m, in \u001b[0;36mDDQNAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mset_priorities(sample_indices, error)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m#* now we fit the main model (q_eval)\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m#* If counter reaches set value, update target network with weights of main network\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m#* it will update it at the very beginning also\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mmem_cntr \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplace_target \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/keras/src/engine/training.py:1770\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1763\u001b[0m (\n\u001b[1;32m   1764\u001b[0m     data_handler\u001b[38;5;241m.\u001b[39m_initial_epoch,\n\u001b[1;32m   1765\u001b[0m     data_handler\u001b[38;5;241m.\u001b[39m_initial_step,\n\u001b[1;32m   1766\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_load_initial_counters_from_ckpt(\n\u001b[1;32m   1767\u001b[0m     steps_per_epoch_inferred, initial_epoch\n\u001b[1;32m   1768\u001b[0m )\n\u001b[1;32m   1769\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1770\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[1;32m   1772\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:1341\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[0;32m-> 1341\u001b[0m     data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1342\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epochs):\n\u001b[1;32m   1343\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:496\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[1;32m    495\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:705\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    701\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    704\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 705\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:744\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    741\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m    742\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[1;32m    743\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[0;32m--> 744\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3420\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   3419\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3420\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3421\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3423\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main loop\n",
    "for episode in trange(500, desc='Test episodes'):\n",
    "\n",
    "        proc.reset()\n",
    "        (observation, info), done = env.reset(), False\n",
    "        truncated = False\n",
    "        proc.update_input(observation, info)\n",
    "        observation = proc.get_input()\n",
    "\n",
    "        episode_reward = 0\n",
    "        episode_len = 0\n",
    "\n",
    "        while (not done) and (not truncated):\n",
    "            \n",
    "            action, action_index = agent.get_action(observation, deterministic=False)\n",
    "            new_observation, reward, done, truncated, new_info = env.step(action=[action])\n",
    "            \n",
    "            proc.update_input(new_observation, info)\n",
    "            new_observation = proc.get_input()\n",
    "\n",
    "            episode_reward += reward # type: ignore\n",
    "            episode_len +=1\n",
    "\n",
    "            if new_info[\"rewards\"][\"on_road_reward\"] == False or episode_len >= 200:\n",
    "                truncated = True\n",
    "            \n",
    "            agent.remember(state=observation, action=action_index, done=done,\n",
    "                            reward=reward, new_state=new_observation)\n",
    "            agent.train()\n",
    "\n",
    "            observation = new_observation\n",
    "\n",
    "        episode_lens.append(episode_len)\n",
    "\n",
    "        score_history.append(episode_reward)\n",
    "        avg_score = np.mean(score_history)\n",
    "        avg_history.append(avg_score)\n",
    "        std_score = np.std(score_history)\n",
    "        std_history.append(std_score)\n",
    "\n",
    "        avg_score_100 = np.mean(score_history[-100:])\n",
    "        avg_history_100.append(avg_score_100)\n",
    "\n",
    "        if avg_score_100 > best_score:\n",
    "            best_score = avg_score\n",
    "            agent.save_model(episode)\n",
    "\n",
    "        agent.tensorboard.update_stats(episode_rew = episode_reward,\n",
    "                                       average_rew =avg_score,\n",
    "                                       average_100_reward = avg_score_100,\n",
    "                                       std_rew=std_score,\n",
    "                                       epsilon=agent.epsilon,\n",
    "                                       episode_len = episode_len)\n",
    "        \n",
    "\n",
    "        print('episode ', episode, 'score %.1f' % episode_reward, 'ep len', episode_len,\n",
    "              'avg score %.1f' % avg_score, 'avg_score_100 %.1f' %avg_score_100,'std score %.1f' % std_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9814\n"
     ]
    }
   ],
   "source": [
    "n_steps = sum(episode_lens)\n",
    "print(n_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/o/Documents/thesis/.venv/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.configure to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.configure` for environment variables or `env.get_wrapper_attr('configure')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b30b2b95434014b3253aeda4512349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test episodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score 106.1 ep len 150\n",
      "Moviepy - Building video /home/o/Documents/thesis/ddqn/videos/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/o/Documents/thesis/ddqn/videos/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/o/Documents/thesis/ddqn/videos/rl-video-episode-0.mp4\n"
     ]
    }
   ],
   "source": [
    "# env = record_videos(env)\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "data = load_config()\n",
    "env = gym.make('racetrack-v0', render_mode = 'rgb_array')\n",
    "env.configure(data) # type: ignore\n",
    "\n",
    "env = RecordVideo(env, video_folder=\"videos\", episode_trigger=lambda e: True)\n",
    "env.unwrapped.set_record_video_wrapper(env)\n",
    "\n",
    "# main loop\n",
    "for episode in trange(1, desc='Test episodes'):\n",
    "\n",
    "        proc.reset()\n",
    "        (observation, info), done = env.reset(), False\n",
    "        truncated = False\n",
    "        proc.update_input(observation, info)\n",
    "        observation = proc.get_input()\n",
    "\n",
    "        episode_reward = 0\n",
    "        episode_len = 0\n",
    "\n",
    "        while (not done) or truncated:\n",
    "            \n",
    "            action, action_index = agent.get_action(observation, deterministic=True)\n",
    "            new_observation, reward, done, truncated, new_info = env.step(action=[action])\n",
    "            \n",
    "            proc.update_input(new_observation, info)\n",
    "            new_observation = proc.get_input()\n",
    "\n",
    "            episode_reward += reward # type: ignore\n",
    "            episode_len +=1\n",
    "\n",
    "            if new_info[\"rewards\"][\"on_road_reward\"] == False or episode_len >=150:\n",
    "               done = True\n",
    "            \n",
    "            observation = new_observation\n",
    "\n",
    "        print('episode ', episode, 'score %.1f' % episode_reward, 'ep len', episode_len)   \n",
    "\n",
    "env.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
